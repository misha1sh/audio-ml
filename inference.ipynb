{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Triton is not available, some optimizations will not be enabled.\n",
      "This is just a warning: No module named 'triton'\n",
      "Either FairScale or torch distributed is not available, MixtureOfExperts will not be exposed. Please install them if you would like to use MoE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=363, out_features=64, bias=True)\n",
       "    (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): xFormerEncoderBlock(\n",
       "      (pose_encoding): SinePositionalEmbedding()\n",
       "      (wrap_att): Residual(\n",
       "        (layer): PreNorm(\n",
       "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (sublayer): MultiHeadDispatch(\n",
       "            (attention): ScaledDotProduct(\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (in_proj_container): InputProjection(\n",
       "              (q_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (k_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (v_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "            )\n",
       "            (resid_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (wrap_ff): PostNorm(\n",
       "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (sublayer): Residual(\n",
       "          (layer): PreNorm(\n",
       "            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (sublayer): MLP(\n",
       "              (mlp): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ReLU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): Flatten(start_dim=1, end_dim=-1)\n",
       "    (5): Linear(in_features=1024, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xformers.factory import xFormerEncoderBlock, xFormerEncoderConfig\n",
    "from pymorphy2.tagset import OpencorporaTag\n",
    "from params import NO_PUNCT, build_params\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "feature_tags_array = [\n",
    "    OpencorporaTag.PARTS_OF_SPEECH, # часть речи\n",
    "    OpencorporaTag.GENDERS, # род\n",
    "    OpencorporaTag.NUMBERS, # число\n",
    "    OpencorporaTag.CASES, # падеж\n",
    "    OpencorporaTag.ASPECTS, # соверш / несоверш\n",
    "    OpencorporaTag.TRANSITIVITY, # перех / непереходный\n",
    "    OpencorporaTag.PERSONS, # лицо\n",
    "    OpencorporaTag.TENSES, # время\n",
    "    OpencorporaTag.MOODS, # наклонение\n",
    "    OpencorporaTag.VOICES, # залог\n",
    "    #INVOLVEMENT\n",
    "    ['Prnt'], # вводные слова\n",
    "    ['Apro'], # местоимение\n",
    "    ['NUMB'], # число вида 1234\n",
    "    ['LATIN'], # текст на английском\n",
    "    ['UNKN'], # неизвестный токен\n",
    "    ['PUNCT_DASH', 'PUNCT_DOT', 'PUNCT_COMMA', 'PUNCT_QUOTE',\n",
    "     'PUNCT_LEFT_PARENTHESIS', 'PUNCT_RIGHT_PARENTHESIS' ], # \"()\n",
    "    ['CAPITALIZED'], # начинается с заглавной буквы\n",
    "    ['Fixd', 'Abbr'] # неизменяемое, сокращение\n",
    "]\n",
    "\n",
    "CUT_NAVEC_TAGS_ARRAY = [\n",
    "    #'NOUN', #'ADJF'\n",
    "]\n",
    "\n",
    "params = build_params({\n",
    "    \"VARIANTS_CNT\": 1,\n",
    "    \"TARGET_CLASSES_COUNT\": 3,\n",
    "    \"INPUT_WORDS_CNT\": 16,\n",
    "    \"feature_tags_array\": feature_tags_array,\n",
    "    \"PUNCTUATION_TARGET\": {\n",
    "        \"$empty\": NO_PUNCT,\n",
    "        \",\": 1,\n",
    "        \".\": 2,\n",
    "        \"!\": 2,\n",
    "        \"?\": 2,\n",
    "    },\n",
    "    \"USE_NAVEC\": True,\n",
    "    'CUT_NAVEC_TAGS_SET': set(CUT_NAVEC_TAGS_ARRAY),\n",
    "    'INFECTED_TEXT_PROB': 0.1,\n",
    "    \"RETAIN_LEFT_PUNCT\": True,\n",
    "})\n",
    "\n",
    "N_words = params[\"INPUT_WORDS_CNT\"]\n",
    "# N_variants = params[\"VARIANTS_CNT\"]\n",
    "N_features = params[\"TOTAL_WORD_FEATURES_CNT\"]\n",
    "\n",
    "INTERNAL_EMBEDDING_SIZE = 64\n",
    "\n",
    "encoder_config = {\n",
    "    \"dim_model\": INTERNAL_EMBEDDING_SIZE, #N_variants * N_features,\n",
    "    \"residual_norm_style\": \"pre\",  # Optional, pre/post\n",
    "    \"position_encoding_config\": {\n",
    "        \"name\": \"sine\",  #sine\n",
    "        # \"dim_model\": VARIANTS_CNT * N_features,\n",
    "    },\n",
    "    \"multi_head_config\": {\n",
    "        \"num_heads\": 4,\n",
    "        \"residual_dropout\": 0.,\n",
    "        \"attention\": {\n",
    "            \"name\": \"scaled_dot_product\", #linformer scaled_dot_product fourier_mix, \"linformer\" scaled_dot_product,  # whatever attention mechanism\n",
    "            \"dropout\": 0., # linformer\n",
    "            \"seq_len\": N_words, # linformer, scaled_dot_product\n",
    "            \"to_seq_len\": N_words, # scaled_dot_product\n",
    "        },\n",
    "    },\n",
    "    \"feedforward_config\": {\n",
    "        \"name\": \"MLP\",\n",
    "        \"dropout\": 0.,\n",
    "        \"activation\": \"relu\",\n",
    "        \"hidden_layer_multiplier\": 1,\n",
    "    },\n",
    "}\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        N_words = params['INPUT_WORDS_CNT']\n",
    "        # N_variants = params['VARIANTS_CNT']\n",
    "        N_features = params['TOTAL_WORD_FEATURES_CNT']\n",
    "\n",
    "        # input is (N, N_words, N_features)\n",
    "        # output is (N, N_words, )\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # nn.Flatten(2), \n",
    "            # (N, N_words, N_features + )\n",
    "            # nn.TransformerEncoder(encoder_layer, num_layers=1),encoder = \n",
    "            nn.Linear(N_features, INTERNAL_EMBEDDING_SIZE),\n",
    "            nn.BatchNorm1d(N_words),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            xFormerEncoderBlock(xFormerEncoderConfig(**encoder_config)),\n",
    "            # xFormerEncoderBlock(xFormerEncoderConfig(**encoder_config)),\n",
    "            # xFormerEncoderBlock(xFormerEncoderConfig(**encoder_config)),\n",
    "            # xFormerEncoderBlock(xFormerEncoderConfig(**encoder_config)),\n",
    "\n",
    "            nn.Flatten(1), # (N, N_words* INTERNAL_EMBEDDING_SIZE)\n",
    "            # nn.Tanh(),\n",
    "            nn.Linear(N_words* INTERNAL_EMBEDDING_SIZE, params['TARGET_CLASSES_COUNT']),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(100, TARGET_CLASSES_COUNT),\n",
    "            # nn.Tanh(),\n",
    "            # nn.Tanhshrink(),\n",
    "            # nn.Sigmoid(),\n",
    "            # nn.ReLU(),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    # buffer_size = 0\n",
    "    # for buffer in model.buffers():\n",
    "    #     buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    # return (param_size + buffer_size) / 1024**2\n",
    "\n",
    "model = torch.load(\"model.v1\", map_location=torch.device('cpu'))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "363"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['TOTAL_WORD_FEATURES_CNT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.1201,  2.5673, -0.3095]])\n"
     ]
    }
   ],
   "source": [
    "dummy_input = torch.ones((1, params['INPUT_WORDS_CNT'], params['TOTAL_WORD_FEATURES_CNT']))\n",
    "with torch.no_grad():\n",
    "        print(model(dummy_input))\n",
    "        # torch.onnx.export(model, dummy_input, \"model.v1.onnx\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/misha-sh/.local/lib/python3.10/site-packages/xformers/components/multi_head_dispatch.py:170: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if query.shape[0] != key.shape[0] or query.shape[0] != value.shape[0]:\n",
      "/home/misha-sh/.local/lib/python3.10/site-packages/xformers/components/multi_head_dispatch.py:180: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if S_Q != S_K:\n",
      "/home/misha-sh/.local/lib/python3.10/site-packages/xformers/components/multi_head_dispatch.py:215: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert (\n",
      "/home/misha-sh/.local/lib/python3.10/site-packages/xformers/components/attention/core.py:223: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  q = q / math.sqrt(k.size(-1))\n"
     ]
    }
   ],
   "source": [
    "torch_script_graph, unconvertible_ops = torch.onnx.utils.unconvertible_ops(\n",
    "    model, dummy_input #, opset_version=opset_version\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unconvertible_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/misha-sh/.local/lib/python3.10/site-packages/torch/onnx/_internal/jit_utils.py:258: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "/home/misha-sh/.local/lib/python3.10/site-packages/torch/onnx/utils.py:687: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "/home/misha-sh/.local/lib/python3.10/site-packages/torch/onnx/utils.py:1172: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n",
      "  params_dict = _C._jit_pass_onnx_constant_fold(\n",
      "/home/misha-sh/.local/lib/python3.10/site-packages/torch/onnx/utils.py:1178: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(model, (dummy_input, ), \"model.v1.onnx\", \n",
    "                input_names = ['inputt'],\n",
    "                output_names = ['output'])#, verbose=True)\n",
    "        # print(torch.onnx.export_to_pretty_string(model, (dummy_input, )))#, verbose=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: tensor([[-2.1201,  2.5673, -0.3095]])\n",
      "onnx: [[-2.1201398  2.5672958 -0.3095349]]\n",
      "torch: tensor([[ 7.6277, -7.6282, -2.7835]])\n",
      "onnx: [[ 7.6277103 -7.6281767 -2.7834713]]\n"
     ]
    }
   ],
   "source": [
    "for a in [dummy_input, np.zeros_like(dummy_input)]:\n",
    "    print('torch:', torch_model_runner(model)(torch.Tensor(a)))\n",
    "    print('onnx:', onnx_model_runner('model.v1.onnx')(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f67ae8b9690>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAicAAAA7CAYAAACg2yoUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAcT0lEQVR4nO2de3AVVx3Hv3sfe9+PPO9NIEkTUoI8bbENV0t1JBKQqdXiiMh0sHaoReio0ArUakpnHBh0dJyWVme04B9abJ0+nEKZtrRQywS0CKUpNALGJg15P28e97F3f/6ROad78wACJLkhv89MJvfunt0957fn/M73/M7ZvQoRERiGYRiGYVIE00RngGEYhmEYxgiLE4ZhGIZhUgoWJwzDMAzDpBQsThiGYRiGSSlYnDAMwzAMk1KwOGEYhmEYJqVgccIwDMMwTErB4oRhGIZhmJSCxQnDMAzDMCkFixOGYRiGYVKKCRMnu3fvxk033QS73Y7S0lL885//nKisMAzDMAyTQkyIOPnrX/+KTZs2oaKiAv/+97+xYMEClJeXo7m5eSKywzAMwzBMCqFMxA//lZaW4rbbbsNTTz0FANB1HXl5eXjooYewdevW8c4OwzAMwzAphGW8LxiLxXDixAls27ZNbjOZTCgrK0NlZeWwx0SjUUSjUfld13W0t7cjIyMDiqKMeZ4ZhmEYhrl2iAjhcBi5ubkwmUaevBl3cdLa2opEIoFAIJC0PRAI4KOPPhr2mB07dmD79u3jkT2GYRiGYcaYuro6TJ8+fcT94y5OroZt27Zh06ZN8ntXVxfy8/NRV1cHr9c7Jtfs6OiAyWSCy+VCLBaDyWSCzWaDrusAcEnFd7VomoaWlhYQEYLBIMLhMADA6/Wis7MTuq4jLS1NRpFsNhs6OzuhaRoyMzOhaRqICKqqore3F4qiwOVyIZFIgIhAROjp6QEAuFwuEBHMZjOICC0tLdB1HcFgEH19fVAUBTabDZqmIRKJwOVywWw2Ix6Pw2w2IxwOIx6PIzMzE7FYDEQEm82GlpYWxONxTJ8+HfF4HEQERVHQ1NQEIkJeXl5SHjo6OuBwOOB0OhGJRGC1WuX53W43TCYTOjo64PP5oOs6IpEI3G43AMhzi7KNdE9EGub6I+zKEczUwXhPBn9OJBKwWCxJ7UHXdekLBhOPx6EoijxG13WZLhaLAQBUVU26jmj3ZrNZ+hexz8jg7+NRl4zlVhQlyZ+L8onvlzsWABKJhLSHOFZRFCiKAk3TAAAWy0A3a/Tb4lhFUWAymaDrOvr6+uB0OmEymeR5NU2DrutQVVXeJ5PJJH200+mU5xG2NtpP3HORD6ONjdcX23Vdl/k3XktcB4DsB6xWa9L5dF0fYrfhtoXDYeTl5cHj8VzyXo27OMnMzITZbEZTU1PS9qamJgSDwWGPsdls8oYa8Xq9YyZOLl68CKvViqysLPT19UFVVaiqitraWthsNuTn58uOejRChYhk4xWNOpFIwGQyQdM0BAIB6LoOj8cDu92ORCIBm80mG4DdbofT6QQwUEksFgsSiQQ8Hk9S5bHZbFAUBaqqykYirkVEcLvdICJYrVbouo7u7m4AgMfjkZXb6XTKY91uNywWC+LxOCwWi2xAHo8H8XgcwEDlb2pqgqZpcLlcUiAJEaUoCjwej2xkQnQlEglZJiECOzs7EYvFkJGRgZ6eHrhcLnR2dqKzsxOBQADRaBTxeBxOpxMtLS3QNA25ubmIx+OIxWJwu91ygXUgEEAsFpMNUeQlLS0NsVgMuq7DZrOhvr4eiqIgJycnSdw1NzdDURRkZWVJe5jNZjQ3NycJOpF3IkJ/fz/MZrNs1GazGX19fdKuwpGZzWZ5T6xW69VU1QkjFouhvr5erhkTYtrpdKKnpwddXV2w2WzIycmRdcRut0sxOVxnONmJx+Oor6+H2WxGdnY2otEoNE2Dz+dDe3s7EokEsrOzAQwMRkwmExRFQX9/v2yXxnYcDoehKArcbjc0TUM8Hpe+sLu7G6qqwuFwoKenR7YjMZAR9V7U70QigXA4LNssEcFisaC/vx+apknRLzpEMTAwmUzIzc2FpmmIxWKw2+0wm83o7u4GEcmBjrinjY2NAAZEixjc6boOh8OBaDSKcDgMVVWRnp6OSCQi03Z3d0NRFHi9XllfjDbw+XyIxWKIRqNwu93o7e1FT08PgsEgiAidnZ3w+/1QFAWNjY3yGn19fUkdcyQSgcPhgMVikW3S5XJBURQ5SBORfaOg6O3tlX7E7/cjHo9D0zTph0U5rVYrrFarvKcOh0P6HeH3RF0Rwk8cJ9IK+4t7I/y7ruuwWCzyHnk8HkQiEenrGxoakJubKweMVqsVjY2NMJlMsi8DBvx5e3s7LBYL3G639Ov9/f0AIPse0Ud5PJ4hg9D09HR0d3fL+tjZ2Yns7GzEYjEplHp6euD1etHW1gav14vW1lZZzy4nQidsQeztt9+OJ598EsCAusrPz8fGjRuvaEFsd3c3fD4furq6xkycGKMHwlmIykJEcDqdsnKZzWa0trbC7/fDarVC0zSoqoq+vj5ZOXVdl6OWtrY2aJomG1UsFoOqqojFYmhoaEAikUBRURG6urpARPD5fKirqwMA5OXlIRKJyDzU1dUhGo2iuLgY/f39Ms/19fWwWCwIBAJJ+WxoaIDVaoXf75cV3dhxut3uIaMJ0bkKNa0oimyIorMR6evq6qAoCgKBACKRCHRdh9PpRFtbGywWCzIyMuSIy2q1orW1FbFYDLm5ubLxWa1WtLS0AACysrIQiUSgqiqampqQSCSQl5eXJIgE19rZiXMKwWYcQQBIKj8RyUiVSC9GGkKACochHI1xpGmsU+3t7dLhiVFKOByGpmlIT08fl058uFHh4H2DR2TRaBQ1NTXQNA0333xzkvNtbW3FxYsX4fF4MHPmTESjUdlJtra2yjoSjUZhMpmGjL6FHYdzYBMRpTHaR5Rd3F/xWQwUhJjNzs5Gf3+/7ERaWlqgKAoyMjIADHSSJpMJVqtV2mdw++vt7QUwtF0a6+KluBJbpXpU0TjCBz6NmA6O+FyJPYwMjqgObuupzHDRsdFwvcpntPml7C/8KTDQf/v9/sv236OKnDz++OND1n6UlJTItSKRSASbN2/Gvn37EI1GUV5ejqeffjppfUltbS3i8Tieeuop/OlPf8I3v/lNmM1m9Pb24r777htNdsYUMZIHkNQoXC6X3G6M5hjLKDpM4XCNjlaMgowVSuw3mUzIyMiQTsgYSgsEAvIcxus6HI6k6SbhOMVoAYCcllIUBT09PbBarUhLS5OjJyJCe3s7TCaTVMiiIxGjOo/HkxQhqqurAxHhpptuSopEiChGMBhELBaTIzLjKNFoCzGNIyIxwIDIMEbRxMhMzE8KoXW9UVVVfjae39jgxGfRMQkGR8+Moknss9vtw6bPysoCADlCAQaigqJBi85PhHnFtQd3mMZ8GsUPADQ2Nso6lp2dDU3TZMTOYrHIKJCIFho738EjayHiRIcsIj6qqspOJC0tDV6vV4b2HQ6HzE9ubq7Mp8PhkOUIh8Noa2sDEaGoqAjhcBgmk0mO1MWUoVHIiusaQ9eDbXKpTkyM/j0ej2w7xuidOG9aWtqIES7RHsU9Ms6jG9MOjgwLew5OJ9qJqAfG7UauVweT6h2x8Z4Cn+Z3sA+4mnJcbqopVRnOJ6VyPox190rzO+ppnTlz5uDNN9/89ASGivPjH/8Y+/fvxwsvvACfz4eNGzfinnvuwdGjRwEMOMwVK1YgGAxiy5YtePbZZ7Fnzx7k5OTg4MGDQxbJXi0jKcnBo+GRFN9ICnqk7YPVt2g0g0f1IvognJkQJcKpWywWOWUinLy4jri5JpMpaW5RCIP09HT09PTIiIqYGxajUlFuv98PTdNgtVqTrhMIBOS1RDRE5E/kV4R1rVYrFEVJCicKx+31eqUY8vv9sux+v1+eT9M0aRvRGQ/u7I12FXmfLI7jajGWzzhfL0SEoihy+kiE1YXw6+jogKqqsNvtsNvtaG9vBzDQ6YkQrojgZGRkSGEqomdiOkrkQ9RhMTdt/G7Mp5jyFNE0ESmqr69HV1cX3G43ZsyYgVgsJqcBhYCdNm1aUmjb5XLJCJ6iKLDb7bKswIAIEPVG5F2EwOvr66WwEkJXiC0x5eHz+QBACjwh9kRaIdREGb1er2xn4n6IvAmxxTDM2DBqcWKxWIZdG9LV1YU//vGP+Mtf/oIvf/nLAIA9e/bgM5/5DI4dO4ZFixbh9ddfx5kzZ/Dmm28iEAhg586d+N3vfoctW7bglltuGXXmw+EwXC5XkrPp7++XCyiNayYAoLe3FxaLBU6nU4bPI5EIWltb5QLOeDwOl8uF1tZWEJGcQwMGHL2YaklPT0c0GpUjWDGtY7FYEIlEYLPZcPHiRRARpk2bht7eXrnAtra2FpqmobCwUC7yFHOeH3/8MXw+H9LS0mTnb7PZ0NDQAF3XkZOTg2g0ilgsBp/PJ6eFzGazzIvVakUikZDzh8IGTqcTDodDiofe3l44nU4QkZzvFesqIpEI7HY7LBYLurq6pNMXTjqRSMgFUWKULf5ER2pcIGYUQsaFUJdarzMVBMnlUBQlKYqXlpYGYKBzFp23xWKBqqpSNBo7TyGI7Xa77ICHE71iDtvYAQtEBE38iTly4/kGH+NwOGTeRd7EGoecnJykcwtRZLVaZVRF5FmUFYAUu6KOi/ICQEFBQZLdjKNqIUpEWuOgQUyxDHfc4BE7wzDjx6hb37lz55Cbmwu73Y5QKIQdO3YgPz8fJ06cQDweR1lZmUw7a9Ys5Ofno7KyEosWLUJlZSXmzZuXFCEpLy/H+vXr8eGHH44oUAa/50Qs3nS5XHLOVjge0QEDSBp1AcnTMCJc6nQ6kZ+fL528ONaYR2M4Pj09fch2IVZE5yscs3C0FotFOkgikmF8s9ksF1iJ0bHH45EdiogkiFC4iGaIUabIg8PhkAJN5MH4lI5xYVZHR4ecGxedjpjTFaNGY0dntIVxRJ2fnw9goEMRDt5isaC4uHhIWiA5hM1cO6LzFvfb2MmKey0wmUzIycmRUQAxbWGcohJTjcYpTPEn6qbxeLGGRkT7xLFC+BjFkPFcIuIBYFghNDjqOdxamMH/GYa58RiVOCktLcXevXtRUlKChoYGbN++HYsXL0ZVVZVcHS1GN4JAICBXbzc2Ng77fhOxbyRGes+JcITXMv92PdYuDDclAQxEasQ6FRHdEIJEdCCiExH5FsIFSB65jRRFECPGwSPN/Pz8pHUsoqyFhYUyvTi/eLLAmI/L2XG4NTnM+DLSvRICwohxrRQw9J4N3i/qs1FUAANTs8ZHRUVdFpEysb6jvb0dXV1dcDqdyMzMlE+tiMWyiURCPsUj6mIikUBbWxsURUkSxCxCGGbqMSpxsnz5cvl5/vz5KC0tRUFBAZ5//vkxnYMd6T0nIoKSqgiHLt7rIRCiQDyuOpYYI04Mc7UYRQkAuXZIREvC4TAikQg0TZOPzrpcLjnNKYRyb2+v9BXi0WPgU4ElRK9xH8MwNw6i377cU0bXNKnq9/sxc+ZMnD9/Hl/5ylcQi8Xks+YC4/tLgsHgkF8fFu87GekdJ8DQ95y0trYCGHislmEYhmGYyUU4HJbLHYbjmsRJT08PLly4gHvvvRcLFy6E1WrFoUOHsHLlSgBAdXU1amtrEQqFAAChUAi/+MUv0NzcLKcR3njjDXi9XsyePfuKryvWfdTW1l6ycMwA3d3dyMvLG9M36t4osK1GB9trdLC9rhy21eiYLPYi+vS3dS7FqMTJww8/jLvuugsFBQW4ePEiKioqYDabsXr1avh8Ptx///3YtGkT0tPT4fV68dBDDyEUCmHRokUAgKVLl2L27Nm49957sWvXLjQ2NuKxxx7Dhg0bhn0D7EiI+XKfz5fSNyHVGMs36t5osK1GB9trdLC9rhy21eiYDPa6kqDCqMTJJ598gtWrV6OtrQ1ZWVm44447cOzYMbmI8ze/+Q1MJhNWrlyZ9BI2gdlsxquvvor169cjFArB5XJh7dq1eOKJJ0ZZNIZhGIZhblQm5PX118p4vL7+RoLtdeWwrUYH22t0sL2uHLbV6LjR7DUpnwG12WyoqKgY1VTQVIbtdeWwrUYH22t0sL2uHLbV6LjR7DUpIycMwzAMw9y4TMrICcMwDMMwNy4sThiGYRiGSSlYnDAMwzAMk1KwOGEYhmEYJqWYdOJk9+7duOmmm2C321FaWjrkdfg3Io8//njST9YrioJZs2bJ/ZFIBBs2bEBGRgbcbjdWrlwpfxZAUFtbixUrVsDpdCI7OxuPPPIINE1LSnP48GHceuutsNlsKC4uxt69e8ejeNfMO++8g7vuugu5ublQFAUvv/xy0n4iws9//nPk5OTA4XCgrKwM586dS0rT3t6ONWvWwOv1wu/34/777x/ym0inT5/G4sWLYbfbkZeXh127dg3JywsvvIBZs2bBbrdj3rx5OHDgwHUv77VyOXt997vfHVLfli1blpRmqthrx44duO222+DxeJCdnY2vf/3rqK6uTkoznu0v1f3fldjrS1/60pD69eCDDyalmQr2euaZZzB//nz50rRQKITXXntN7p/y9YomEfv27SNVVenZZ5+lDz/8kNatW0d+v5+ampomOmtjSkVFBc2ZM4caGhrkX0tLi9z/4IMPUl5eHh06dIjee+89WrRoEX3+85+X+zVNo7lz51JZWRmdPHmSDhw4QJmZmbRt2zaZ5r///S85nU7atGkTnTlzhp588kkym8108ODBcS3r1XDgwAH66U9/Si+++CIBoJdeeilp/86dO8nn89HLL79M77//Pn3ta1+jwsJC6u/vl2mWLVtGCxYsoGPHjtE//vEPKi4uptWrV8v9XV1dFAgEaM2aNVRVVUXPPfccORwO+v3vfy/THD16lMxmM+3atYvOnDlDjz32GFmtVvrggw/G3Aaj4XL2Wrt2LS1btiypvrW3tyelmSr2Ki8vpz179lBVVRWdOnWKvvrVr1J+fj719PTINOPV/iaD/7sSe33xi1+kdevWJdWvrq4uuX+q2Ovvf/877d+/n/7zn/9QdXU1Pfroo2S1WqmqqoqIuF5NKnFy++2304YNG+T3RCJBubm5tGPHjgnM1dhTUVFBCxYsGHZfZ2cnWa1WeuGFF+S2s2fPEgCqrKwkooHOyGQyUWNjo0zzzDPPkNfrpWg0SkREP/nJT2jOnDlJ5161ahWVl5df59KMLYM7W13XKRgM0i9/+Uu5rbOzk2w2Gz333HNERHTmzBkCQP/6179kmtdee40URaH6+noiInr66acpLS1N2ouIaMuWLVRSUiK/f+tb36IVK1Yk5ae0tJS+//3vX9cyXk9GEid33333iMdMZXs1NzcTADpy5AgRjW/7m4z+b7C9iAbEyQ9/+MMRj5nK9kpLS6M//OEPXK+IaNJM68RiMZw4cQJlZWVym8lkQllZGSorKycwZ+PDuXPnkJubi6KiIqxZswa1tbUAgBMnTiAejyfZZdasWcjPz5d2qaysxLx58xAIBGSa8vJydHd348MPP5RpjOcQaSa7bWtqatDY2JhUNp/Ph9LS0iT7+P1+fO5zn5NpysrKYDKZcPz4cZnmzjvvhKqqMk15eTmqq6vR0dEh09woNjx8+DCys7NRUlKC9evXo62tTe6byvbq6uoC8OmPj45X+5us/m+wvQR//vOfkZmZiblz52Lbtm3o6+uT+6aivRKJBPbt24fe3l6EQiGuV7jGXyUeT1pbW5FIJJJuBAAEAgF89NFHE5Sr8aG0tBR79+5FSUkJGhoasH37dixevBhVVVVobGyEqqrw+/1JxwQCATQ2NgIAGhsbh7Wb2HepNN3d3ejv74fD4Rij0o0tonzDlc1YdvEr2QKLxYL09PSkNIWFhUPOIfalpaWNaENxjsnCsmXLcM8996CwsBAXLlzAo48+iuXLl6OyshJms3nK2kvXdfzoRz/CF77wBcydOxcAxq39dXR0TDr/N5y9AOA73/kOCgoKkJubi9OnT2PLli2orq7Giy++CGBq2euDDz5AKBRCJBKB2+3GSy+9hNmzZ+PUqVNTvl5NGnEylVm+fLn8PH/+fJSWlqKgoADPP//8pBUNTOry7W9/W36eN28e5s+fjxkzZuDw4cNYsmTJBOZsYtmwYQOqqqrw7rvvTnRWJgUj2euBBx6Qn+fNm4ecnBwsWbIEFy5cwIwZM8Y7mxNKSUkJTp06ha6uLvztb3/D2rVrceTIkYnOVkowaaZ1MjMzYTabh6xWbmpqQjAYnKBcTQx+vx8zZ87E+fPnEQwGEYvF0NnZmZTGaJdgMDis3cS+S6Xxer2TWgCJ8l2q3gSDQTQ3Nyft1zQN7e3t18WGk71+FhUVITMzE+fPnwcwNe21ceNGvPrqq3j77bcxffp0uX282t9k838j2Ws4SktLASCpfk0Ve6mqiuLiYixcuBA7duzAggUL8Nvf/pbrFSaROFFVFQsXLsShQ4fkNl3XcejQIYRCoQnM2fjT09ODCxcuICcnBwsXLoTVak2yS3V1NWpra6VdQqEQPvjgg6QO5Y033oDX68Xs2bNlGuM5RJrJbtvCwkIEg8GksnV3d+P48eNJ9uns7MSJEydkmrfeegu6rkvHGQqF8M477yAej8s0b7zxBkpKSpCWlibT3Ig2/OSTT9DW1oacnBwAU8teRISNGzfipZdewltvvTVkqmq82t9k8X+Xs9dwnDp1CgCS6tdUsddgdF1HNBrlegVMvkeJbTYb7d27l86cOUMPPPAA+f3+pNXKNyKbN2+mw4cPU01NDR09epTKysooMzOTmpubiWjgkbP8/Hx666236L333qNQKEShUEgeLx45W7p0KZ06dYoOHjxIWVlZwz5y9sgjj9DZs2dp9+7dk+ZR4nA4TCdPnqSTJ08SAPr1r39NJ0+epI8//piIBh4l9vv99Morr9Dp06fp7rvvHvZR4ltuuYWOHz9O7777Lt18881Jj8Z2dnZSIBCge++9l6qqqmjfvn3kdDqHPBprsVjoV7/6FZ09e5YqKipS7tFYokvbKxwO08MPP0yVlZVUU1NDb775Jt1666108803UyQSkeeYKvZav349+Xw+Onz4cNKjr319fTLNeLW/yeD/Lmev8+fP0xNPPEHvvfce1dTU0CuvvEJFRUV05513ynNMFXtt3bqVjhw5QjU1NXT69GnaunUrKYpCr7/+OhFxvZpU4oSI6Mknn6T8/HxSVZVuv/12Onbs2ERnacxZtWoV5eTkkKqqNG3aNFq1ahWdP39e7u/v76cf/OAHlJaWRk6nk77xjW9QQ0ND0jn+97//0fLly8nhcFBmZiZt3ryZ4vF4Upq3336bPvvZz5KqqlRUVER79uwZj+JdM2+//TYBGPK3du1aIhp4nPhnP/sZBQIBstlstGTJEqqurk46R1tbG61evZrcbjd5vV667777KBwOJ6V5//336Y477iCbzUbTpk2jnTt3DsnL888/TzNnziRVVWnOnDm0f//+MSv31XIpe/X19dHSpUspKyuLrFYrFRQU0Lp164Y4qqlir+HsBCCpbYxn+0t1/3c5e9XW1tKdd95J6enpZLPZqLi4mB555JGk95wQTQ17fe9736OCggJSVZWysrJoyZIlUpgQcb1SiIjGL07DMAzDMAxzaSbNmhOGYRiGYaYGLE4YhmEYhkkpWJwwDMMwDJNSsDhhGIZhGCalYHHCMAzDMExKweKEYRiGYZiUgsUJwzAMwzApBYsThmEYhmFSChYnDMMwDMOkFCxOGIZhGIZJKVicMAzDMAyTUrA4YRiGYRgmpfg/3cGbEVMTUGMAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import onnx\n",
    "onnx_model = onnx.load(\"model.v1.onnx\")\n",
    "\n",
    "\n",
    "from onnx.tools.net_drawer import GetOpNodeProducer, GetPydotGraph\n",
    "\n",
    "pydot_graph = GetPydotGraph(\n",
    "    onnx_model.graph, name=onnx_model.graph.name, rankdir=\"LR\", node_producer=GetOpNodeProducer(\"docstring\")\n",
    ")\n",
    "pydot_graph.write_dot(\"graph.dot\")\n",
    "\n",
    "import os\n",
    "os.system(\"dot -O -Tsvg graph.dot\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = plt.imread(\"graph.dot.png\")\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-2.1201398,  2.5672958, -0.3095349]], dtype=float32)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Отмечается, что 44-летняя россиянка, первый человек, которому предъявят обвинение в попытке вмешаться в выборы 2018 года. По информации Минюста, она и еще несколько человек выдавала себя за американскую политическую активистку и скрывала свое российское происхождение.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymorphy2\n",
    "from dataset_builder import calculate_word_features_for_tokens, get_word_features, PAD_TOKEN\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "def torch_model_runner(model):\n",
    "    model.eval()\n",
    "    def func(input):\n",
    "        with torch.no_grad():\n",
    "            return model(input)\n",
    "    return func\n",
    "\n",
    "def onnx_model_runner(path):\n",
    "    ort_sess = ort.InferenceSession(path)\n",
    "    def func(input):\n",
    "        return ort_sess.run(None, {'inputt': np.array(input) })[0]\n",
    "    return func\n",
    "\n",
    "def infer(model, text):\n",
    "    assert params[\"RETAIN_LEFT_PUNCT\"] # \n",
    "\n",
    "    unpadded_tokens = text.split(' ')\n",
    "    unpadded_tokens = list(filter(lambda x: len(x) > 0, unpadded_tokens))\n",
    "    tokens = [PAD_TOKEN] * params['INPUT_WORDS_CNT_LEFT'] + unpadded_tokens + [PAD_TOKEN] * (params[\"INPUT_WORDS_CNT_RIGHT\"] + 1)\n",
    "    features = calculate_word_features_for_tokens(tokens, params)\n",
    "\n",
    "    res = \"\"\n",
    "\n",
    "    i = params['INPUT_WORDS_CNT_LEFT']\n",
    "    while i < len(tokens) - params['INPUT_WORDS_CNT_RIGHT']:\n",
    "        tokens_for_batch = tokens[i - params['INPUT_WORDS_CNT_LEFT']: i + params['INPUT_WORDS_CNT_RIGHT']]\n",
    "\n",
    "        tokens_for_batch_copy = tokens_for_batch.copy()\n",
    "        tokens_for_batch_copy.insert(params['INPUT_WORDS_CNT_LEFT'], '?')\n",
    "        # print(\" \".join(tokens_for_batch_copy))\n",
    "\n",
    "\n",
    "        features_for_batch = features[i - params['INPUT_WORDS_CNT_LEFT']: i + params['INPUT_WORDS_CNT_RIGHT']]\n",
    "        features_for_batch = torch.stack((features_for_batch, ))\n",
    "        output_probs = model(features_for_batch)\n",
    "        punct_idx = np.argmax(output_probs).item()\n",
    "        punct = params[\"ID_TO_PUNCTUATION\"][punct_idx]\n",
    "\n",
    "        # print(punct)\n",
    "\n",
    "        # punct = '.'\n",
    "\n",
    "        if punct != '$empty':\n",
    "            res += punct\n",
    "            if tokens[i] != 'PAD':\n",
    "                res += \" \" + tokens[i]\n",
    "            tokens.insert(i, punct)\n",
    "            features = torch.cat((features[:i], \n",
    "                    torch.stack((get_word_features(punct, params), )), \n",
    "                    features[i:]), 0)\n",
    "            i += 2\n",
    "        else:\n",
    "            if tokens[i] != 'PAD':\n",
    "                res += \" \" + tokens[i]\n",
    "            i += 1\n",
    "\n",
    "    return res.strip()\n",
    "\n",
    "\n",
    "infer(onnx_model_runner(\"model.v1.onnx\"), \"Отмечается что 44-летняя россиянка первый человек которому предъявят обвинение в попытке вмешаться в выборы 2018 года По информации Минюста она и еще несколько человек выдавала себя за американскую политическую активистку и скрывала свое российское происхождение\")\n",
    "# print(infer(torch_model_runner(model), \"Я пришел домой Мама сварила суп\"))\n",
    "# https://onnxruntime.ai/docs/reference/ort-format-models.html\n",
    "\n",
    "# infer(onnx_model_runner(\"model.v1.onnx\"), \"Я пришел домой Мама сварила суп\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parse(word='фывфывфов', tag=OpencorporaTag('NOUN,inan,masc plur,gent'), normal_form='фывфывф', score=1.0, methods_stack=((FakeDictionary(), 'фывфывфов', 34, 7), (KnownSuffixAnalyzer(min_word_length=4, score_multiplier=0.5), 'фы')))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "morph.parse(\"фывфывфы\")[0].inflect({'gent'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Мама, обычно холодная, как оконное стекло, была в такой ярости, что переколотила дома всю посуду. Хотела даже просить расчёт, но опомнилась: в других ресторанах и своих певиц хватало. Потому мама просто перестала брать Саню с собой, и он впервые оказался предоставлен самому себе. Поначалу Саня робел один выйти наружу, скучал, слонялся, не зная, чем заняться, — то по комнатам, то по двору. Но к весне осмелел настолько, что исследовал сперва окрестные улицы, а потом и весь обитаемый воронежский мир.  Мир этот оказался полон женщинами.  Саня, разинув от восхищения рот, смотрел на проплывающих по Большой Дворянской разодетых дам, похожих на вазы с фруктами и цветами. На гимназисток, вечно державшихся дрожащими стайками, словно мотыльки. На горничных девушек, спешащих по хозяйкиным делам и прошивающих город мелкими аккуратными стежками.  Саня влюбился во всех них сразу — и навсегда.\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Кристина Герасимова (к концу десятилетия, кажется, поменявшая фамилию на Луна) к концу 2010-х оказалась одним из самых работоспособных музыкантов в русской поп-музыке (впрочем, и не только поп), выйдя на уровень, при котором она смогла себе позволить выпускать по альбому в год без потерь в качестве. Лучший из них, впрочем, «Заколдованные сны», как будто бы нарушал классическую для нее схему и уходил то в чистый и практически бессловесный электропоп, то в ностальгию по неприхотливой русской поп-музыке нулевых, то в постпанк — при этом всегда оставаясь песнями самой Луны.\n",
    "\"\"\"\n",
    "text = \"\"\"\n",
    "\n",
    "Мама, обычно холодная, как оконное стекло, была в такой ярости, что переколотила дома всю посуду. Хотела даже просить расчёт, но опомнилась: в других ресторанах и своих певиц хватало. Потому мама просто перестала брать Саню с собой, и он впервые оказался предоставлен самому себе. Поначалу Саня робел один выйти наружу, скучал, слонялся, не зная, чем заняться, — то по комнатам, то по двору. Но к весне осмелел настолько, что исследовал сперва окрестные улицы, а потом и весь обитаемый воронежский мир.\n",
    "\n",
    "Мир этот оказался полон женщинами.\n",
    "\n",
    "Саня, разинув от восхищения рот, смотрел на проплывающих по Большой Дворянской разодетых дам, похожих на вазы с фруктами и цветами. На гимназисток, вечно державшихся дрожащими стайками, словно мотыльки. На горничных девушек, спешащих по хозяйкиным делам и прошивающих город мелкими аккуратными стежками.\n",
    "\n",
    "Саня влюбился во всех них сразу — и навсегда.\"\"\".replace(\"\\n\", \" \")\n",
    "print(text)\n",
    "\n",
    "text = text.replace(\".\", \"\").replace(\",\", \"\").replace(\"\\n\", \" \")\n",
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m count_parameters\n\u001b[0;32m----> 2\u001b[0m count_parameters(model)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172.7963150459843 tokens per second\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "start = time()\n",
    "reps = 100\n",
    "for i in range(0, reps):\n",
    "    infer(onnx_model_runner(\"model.v1.onnx\"), text)\n",
    "end = time()\n",
    "\n",
    "print(1 / ((end - start) / reps / len(text.split(' '))) / 8, 'tokens per second')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129.86710276061967 tokens per second\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "dummy_input = torch.ones((1, params['INPUT_WORDS_CNT'], params['TOTAL_WORD_FEATURES_CNT']))\n",
    "model.eval()\n",
    "\n",
    "start = time()\n",
    "reps = 10000\n",
    "for i in range(0, reps):\n",
    "    with torch.no_grad():\n",
    "        model(dummy_input)\n",
    "end = time()\n",
    "\n",
    "print(1 / ((end - start) / reps) / 8, 'tokens per second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19739913940429688"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import count_parameters\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'Model' on <module '__main__'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39mmodel.v1\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m model\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:789\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    787\u001b[0m             \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    788\u001b[0m                 \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 789\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m    790\u001b[0m \u001b[39mif\u001b[39;00m weights_only:\n\u001b[1;32m    791\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:1131\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1130\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1131\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1133\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1135\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:1124\u001b[0m, in \u001b[0;36m_load.<locals>.UnpicklerWrapper.find_class\u001b[0;34m(self, mod_name, name)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m mod_name \u001b[39m=\u001b[39m load_module_mapping\u001b[39m.\u001b[39mget(mod_name, mod_name)\n\u001b[0;32m-> 1124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfind_class(mod_name, name)\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't get attribute 'Model' on <module '__main__'>"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model = torch.load(\"model.v1\")\n",
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
