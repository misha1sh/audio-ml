import zipfile

def pack_lib():
    dir_to_zip = "../lib"
    zip_file_name = "../lib.zip"
    if os.path.exists(zip_file_name):
        os.remove(zip_file_name)
    exclude_dirs = ["cache"]
    with zipfile.ZipFile(zip_file_name, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for root, dirs, files in os.walk(dir_to_zip):
            dirs[:] = [d for d in dirs if d not in exclude_dirs]
            for file in files:
                zipf.write(os.path.join(root, file), arcname="lib/" + os.path.relpath(os.path.join(root, file), dir_to_zip))
    return zip_file_name
pack_lib()


# def check_and_upload_file(client, local_filepath, filename):
#     # Function to check if the file exists on the worker and upload it if necessary
#     def _check_file_exists(filename):
#         return os.path.exists(filename)

#     # Check if the file exists on each worker
#     file_exists = client.run(_check_file_exists, filename)
#     print(file_exists)
#     # If the file doesn't exist on any worker, upload it
#     if not all(file_exists.values()):
#         client.upload_file(local_filepath)
#         print(f"Uploaded {filename} to all workers.")
#     else:
#         print(f"{filename} already exists on all workers.")

# check_and_upload_file(client, "./cache/lenta-ru-news.csv.gz", "lenta-ru-news.csv.gz")

# nn.Tanhshrink(),
# nn.Sigmoid(),
# nn.ReLU(),

import importlib
import trainer_mod
import tuner

importlib.reload(trainer_mod)
importlib.reload(tuner)
Trainer = trainer_mod.Trainer

def remap(val, old_0, old_1, new_0, new_1):
    return (val - old_0) / (old_1 - old_0) * (new_1 - new_0) + new_0

def suggest_activation(name, trial):
    cat = trial.suggest_categorical(name, ["tanh", "relu", "leakyrelu", "sigmoid"])
    if cat == "tanh":
        return nn.Tanh()
    elif cat == "relu":
        return nn.ReLU(True)
    elif cat == "leakyrelu":
        return nn.LeakyReLU(trial.suggest_float("leakyrelu",0.01, 0.01, 0.1, log=True))
    elif cat == "sigmoid":
        return nn.Sigmoid()

class Autoencoder(nn.Module):
    def __init__(self, max_val, trial):
        super().__init__()

        self.max_val = max_val

        mk = trial.suggest_int("mid_kernels", 2, 2, 10)

        if trial.suggest_int("batchnorm", 1, 0, 1) == 1:
            batch_normalization = lambda channels: nn.BatchNorm2d(channels)
        else:
            batch_normalization = lambda channels: nn.Identity()

        self.encoder = nn.Sequential( # 1, 128, 9
            nn.Conv2d(1, mk, kernel_size=(8, 9), padding="same"),  # mk, 128, 9
            batch_normalization(mk),
            suggest_activation("act_enc1", trial),
            nn.MaxPool2d(kernel_size=(4, 1)),  # mk, 32, 9

            nn.Conv2d(mk, mk, kernel_size=(8, 3), padding="same"), # mk, 32, 9
            batch_normalization(mk),
            suggest_activation("act_enc2", trial),
            nn.MaxPool2d(kernel_size=(2, 1)),  # mk, 8, 9

            nn.Conv2d(mk, 4, kernel_size=(4, 3), padding="same"), # 4, 8, 9
            batch_normalization(4),
            suggest_activation("act_enc3", trial),
            nn.MaxPool2d(kernel_size=(2, 1)),  # 4, 8, 9

            nn.Flatten(),
            nn.Linear(4 * 8 * 9),
            nn.Tanh(),
            nn.Unflatten(1, (4, 8, 9))
        )

        self.decoder = nn.Sequential(
            nn.Conv2d(4, mk, kernel_size=(4, 3), padding="same"), # mk, 8, 9
            suggest_activation("act_dec1", trial),
            batch_normalization(mk),
            nn.UpsamplingNearest2d(scale_factor=(2, 1)),  # mk, 16, 9

            nn.Conv2d(mk, mk, kernel_size=(4, 3), padding="same"), # mk, 16, 9
            suggest_activation("act_dec2", trial),
            batch_normalization(mk),
            nn.UpsamplingNearest2d(scale_factor=(2, 1)),  # mk, 32, 9

            nn.Conv2d(mk, mk, kernel_size=(8, 3), padding="same"), # mk, 32, 9
            suggest_activation("act_dec3", trial),
            batch_normalization(mk),
            nn.UpsamplingNearest2d(scale_factor=(4, 1)),  # mk, 128, 9

            nn.Conv2d(mk, 1, kernel_size=(16, 3), padding="same"), # 1, 128, 9
            suggest_activation("act_dec4", trial),
        )

    def forward(self, x):
        encoded = self.encoder(x / self.max_val)
        decoded = self.decoder(encoded)
        return decoded * self.max_val

def accurancy(trainer):
    real = trainer.x_test
    with torch.no_grad():
        predicted = trainer.model(real)

    accurancy = ((real - predicted).abs()).mean().item()

    return {
        "accurancy": accurancy,
    }

def calc_loss_on_data(model, loss, x_gt):
    x_pred = model(x_gt)
    return loss(x_gt, x_pred)


def create_trainer(trial, x):
    max_val = x.max().item()
    model = Autoencoder(max_val, trial)

    optimizer = torch.optim.Adam(model.parameters(),
                            lr=trial.suggest_float("lr", 7e-2, 1e-3, 1e-1, log=True),
                            betas=(0.5, 0.999))
    trainer = Trainer(model=model,
                    loss=nn.MSELoss(),
                    optimizer=optimizer,
                    scheduler=ReduceLROnPlateau(optimizer, factor=0.2, threshold=1e-5, patience=50),
                    sample_count=-1,
                    calc_loss_on_data=calc_loss_on_data,
                    additional_losses={
                        "accurancy": accurancy,
                    })

    x_prepared = torch.unsqueeze(x, 1) # Add channel
    trainer.set_data(x_prepared, test_count=50)

    # trainer.data_lambda = get_noised
    trainer.early_stop_lambda = lambda trainer: trainer.get_lr() < 1e-5
    return trainer


def objective(trial, x):
    trainer = create_trainer(trial, x)
    trainer.train(100, trial=trial, log=False)
    # trainer.plot_history(100)

    return trainer.history['test_loss'][-1]

# cuda = torch.device('cuda:0')
# tuned_params = tuner.tune(objective, n_trials=1000, timeout=60000)
# tuned_params

trial.params['batchnorm'] = 0
trial.params['lr'] = 0.005
trial.params['mid_kernels'] = 8
trainer = create_trainer(trial, x)
trainer.scheduler = ReduceLROnPlateau(trainer.optimizer, factor=0.7, threshold=1e-2, patience=20)
trainer.train(5000, trial=trial, log=True)
trainer.plot_history(100)
