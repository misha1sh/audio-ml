{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dc6e813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imports\n",
    "import importlib\n",
    "importlib.reload(imports)\n",
    "from imports import *\n",
    "\n",
    "# https://pymorphy2.readthedocs.io/en/stable/user/grammemes.html\n",
    "# http://opencorpora.org/dict.php?act=gram\n",
    "# https://github.com/pymorphy2/pymorphy2/blob/92d546f042ff14601376d3646242908d5ab786c1/pymorphy2/tagset.py#L130\n",
    "feature_tags_array = [\n",
    "    OpencorporaTag.PARTS_OF_SPEECH, # часть речи\n",
    "    OpencorporaTag.GENDERS, # род\n",
    "    OpencorporaTag.NUMBERS, # число\n",
    "    OpencorporaTag.CASES, # падеж\n",
    "    OpencorporaTag.ASPECTS, # соверш / несоверш\n",
    "    OpencorporaTag.TRANSITIVITY, # перех / непереходный\n",
    "    OpencorporaTag.PERSONS, # лицо\n",
    "    OpencorporaTag.TENSES, # время\n",
    "    OpencorporaTag.MOODS, # наклонение\n",
    "    OpencorporaTag.VOICES, # залог\n",
    "    #INVOLVEMENT\n",
    "    ['Prnt'], # вводные слова\n",
    "    ['Apro'], # местоимение\n",
    "    ['NUMB'], # число вида 1234\n",
    "    ['LATIN'], # текст на английском\n",
    "    ['UNKN'], # неизвестный токен\n",
    "    ['PUNCT_DASH', 'PUNCT_DOT', 'PUNCT_COMMA', 'PUNCT_QUOTE',\n",
    "     'PUNCT_LEFT_PARENTHESIS', 'PUNCT_RIGHT_PARENTHESIS' ], # \"()\n",
    "    ['CAPITALIZED'], # начинается с заглавной буквы\n",
    "    ['Fixd', 'Abbr'] # неизменяемое, сокращение\n",
    "]\n",
    "\n",
    "CUT_NAVEC_TAGS_ARRAY = [\n",
    "    #'NOUN', #'ADJF'\n",
    "]\n",
    "\n",
    "params = build_params({\n",
    "    \"VARIANTS_CNT\": 1,\n",
    "    \"TARGET_CLASSES_COUNT\": 3,\n",
    "    \"INPUT_WORDS_CNT\": 16,\n",
    "    \"feature_tags_array\": feature_tags_array,\n",
    "    \"PUNCTUATION_TARGET\": {\n",
    "        \"$empty\": NO_PUNCT,\n",
    "        \",\": 1,\n",
    "        \".\": 2,\n",
    "        \"!\": 2,\n",
    "        \"?\": 2,\n",
    "    },\n",
    "    \"USE_NAVEC\": True,\n",
    "    'CUT_NAVEC_TAGS_SET': set(CUT_NAVEC_TAGS_ARRAY),\n",
    "    'INFECTED_TEXT_PROB': 0.1,\n",
    "    \"RETAIN_LEFT_PUNCT\": True,\n",
    "    'type': 'lenta',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7373823a-aabb-4da3-93d7-88b3a8734740",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "server = run_server_if_not_running()\n",
    "server_install_packages(server)\n",
    "\n",
    "# server.rpc_simple(dataset_builder.get_word_features, 'кошка', params).shape\n",
    "# server.rpc_simple(dataset_builder.create_dataset, ['а, б'], params, False)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server is listening on 0.0.0.0:65231\n",
      "Connected by ('51.250.1.213', 52740)\n",
      "write_task started\n",
      "submit task\n",
      "submit task\n",
      "submit task\n",
      "chunk_loaded_callback\n",
      "getting result started\n",
      "BasicAsyncResult completed with result `(tensor([[[ 0.0000, ...`\n",
      "writing started\n",
      "1423.3969116210938 MB\n",
      "writing finishedsubmit task\n",
      "\n",
      "chunks_count 1\n",
      "chunk_loaded_callback\n",
      "getting result started\n",
      "BasicAsyncResult completed with result `(tensor([[[ 0.0000, ...`\n",
      "writing started\n",
      "1232.7687377929688 MB\n",
      "writing finishedsubmit task\n",
      "\n",
      "chunks_count 2\n",
      "chunk_loaded_callback\n",
      "getting result started\n",
      "BasicAsyncResult completed with result `(tensor([[[ 0.0000, ...`\n",
      "writing started\n",
      "1088.025146484375 MB\n",
      "writing finishedsubmit task\n",
      "\n",
      "chunks_count 3\n",
      "chunk_loaded_callback\n",
      "getting result started\n",
      "BasicAsyncResult completed with result `(tensor([[[ 0.0000, ...`\n",
      "writing started\n",
      "1055.2235412597656 MB\n",
      "writing finishedsubmit task\n",
      "\n",
      "chunks_count 4\n",
      "chunk_loaded_callback\n",
      "getting result started\n",
      "BasicAsyncResult completed with result `(tensor([[[ 0.0000, ...`\n",
      "writing started\n",
      "1106.3368835449219 MB\n",
      "writing finishedsubmit task\n",
      "\n",
      "chunks_count 5\n",
      "chunk_loaded_callback\n",
      "getting result started\n",
      "BasicAsyncResult completed with result `(tensor([[[ 0.0000, ...`\n",
      "writing started\n",
      "1147.31396484375 MB\n",
      "writing finishedsubmit task\n",
      "\n",
      "chunks_count 6\n",
      "chunk_loaded_callback\n",
      "getting result started\n",
      "BasicAsyncResult completed with result `(tensor([[[ 0.0000, ...`\n",
      "writing started\n",
      "1142.4507751464844 MB\n",
      "chunk_loaded_callback\n",
      "getting result started\n",
      "BasicAsyncResult completed with result `(tensor([[[ 0.0000, ...`\n",
      "writing started\n",
      "1161.8370666503906 MB\n",
      "writing finishedsubmit task\n",
      "\n",
      "chunks_count 7\n",
      "chunk_loaded_callback\n",
      "getting result started\n",
      "BasicAsyncResult completed with result `(tensor([[[ 0.0000, ...`\n",
      "writing started\n",
      "1131.6498413085938 MB\n",
      "chunk_loaded_callback\n",
      "getting result started\n",
      "BasicAsyncResult completed with result `(tensor([[[ 0.0000, ...`\n",
      "writing started\n",
      "1126.7977294921875 MB\n",
      "writing finishedsubmit task\n",
      "\n",
      "chunks_count 8\n",
      "chunk_loaded_callback\n",
      "getting result started\n",
      "BasicAsyncResult completed with result `(tensor([[[ 0.0000, ...`\n",
      "writing started\n",
      "1109.1395874023438 MB\n",
      "writing finishedsubmit task\n",
      "\n",
      "chunks_count 9\n",
      "chunk_loaded_callback\n",
      "getting result started\n",
      "BasicAsyncResult completed with result `(tensor([[[ 0.0000, ...`\n",
      "writing started\n",
      "1122.5105895996094 MB\n",
      "writing finishedsubmit task\n",
      "\n",
      "chunks_count 10\n",
      "chunk_loaded_callback\n",
      "getting result started\n",
      "BasicAsyncResult completed with result `(tensor([[[ 0.0000, ...`\n",
      "writing started\n",
      "1215.02197265625 MB\n",
      "writing finishedsubmit task\n",
      "\n",
      "chunks_count 11\n",
      "chunk_loaded_callback\n",
      "getting result started\n",
      "BasicAsyncResult completed with result `(tensor([[[ 0.0000, ...`\n",
      "writing started\n",
      "1216.8498229980469 MB\n",
      "writing finishedsubmit task\n",
      "\n",
      "chunks_count 12\n",
      "chunk_loaded_callback\n",
      "getting result started\n",
      "BasicAsyncResult completed with result `(tensor([[[ 0.0000, ...`\n",
      "writing started\n",
      "1237.4768371582031 MB\n",
      "writing finishedsubmit task\n",
      "\n",
      "chunks_count 13\n",
      "chunk_loaded_callback\n",
      "getting result started\n",
      "BasicAsyncResult completed with result `(tensor([[[ 0.0000, ...`\n",
      "writing started\n",
      "1328.1382141113281 MB\n",
      "writing finishedsubmit task\n",
      "\n",
      "chunks_count 14\n",
      "chunk_loaded_callback\n",
      "getting result started\n",
      "BasicAsyncResult completed with result `(tensor([[[ 0.0000, ...`\n",
      "writing started\n",
      "1265.802978515625 MB\n",
      "writing finishedsubmit task\n",
      "\n",
      "chunks_count 15\n",
      "chunk_loaded_callback\n",
      "getting result started\n",
      "BasicAsyncResult completed with result `(tensor([[[ 0.0000, ...`\n",
      "writing started\n",
      "1305.7498168945312 MB\n",
      "writing finished\n",
      "chunks_count 16\n",
      "submit task\n",
      "chunk_loaded_callback\n",
      "getting result started\n",
      "BasicAsyncResult completed with result `(tensor([[[ 0.0000, ...`\n",
      "writing started\n",
      "1396.433349609375 MB\n",
      "writing finishedsubmit task\n",
      "\n",
      "chunks_count 17\n",
      "chunk_loaded_callback\n",
      "getting result started\n",
      "BasicAsyncResult completed with result `(tensor([[[ 0.0000, ...`\n",
      "writing started\n",
      "1395.0929260253906 MB\n",
      "writing finishedsubmit task\n",
      "\n",
      "chunks_count 18\n",
      "chunk_loaded_callback\n",
      "getting result started\n",
      "BasicAsyncResult completed with result `(tensor([[[ 0.0000, ...`\n",
      "writing started\n",
      "1332.00439453125 MB\n",
      "writing finishedsubmit task\n",
      "\n",
      "chunks_count 19\n",
      "chunk_loaded_callback\n",
      "getting result started\n",
      "BasicAsyncResult completed with result `(tensor([[[ 0.0000, ...`\n",
      "writing started\n",
      "1364.5068969726562 MB\n",
      "writing finishedsubmit task\n",
      "\n",
      "chunks_count 20\n",
      "chunk_loaded_callback\n",
      "getting result started\n",
      "BasicAsyncResult completed with result `(tensor([[[ 0.0000, ...`\n",
      "writing started\n",
      "1297.9842224121094 MB\n",
      "writing finishedsubmit task\n",
      "\n",
      "chunks_count 21\n",
      "chunk_loaded_callback\n",
      "getting result started\n",
      "BasicAsyncResult completed with result `(tensor([[[ 0.0000, ...`\n",
      "writing started\n",
      "1384.5246276855469 MB\n",
      "writing finishedsubmit task\n",
      "\n",
      "chunks_count 22\n",
      "chunk_loaded_callback\n",
      "getting result started\n",
      "BasicAsyncResult completed with result `(tensor([[[ 0.0000, ...`\n",
      "writing started\n",
      "1459.1895446777344 MB\n",
      "writing finishedsubmit task\n",
      "\n",
      "chunks_count 23\n",
      "chunk_loaded_callback\n",
      "getting result started\n",
      "BasicAsyncResult completed with result `(tensor([[[ 0.0000, ...`\n",
      "writing started\n",
      "1417.5145568847656 MB\n",
      "writing finishedsubmit task\n",
      "\n",
      "chunks_count 24\n",
      "chunk_loaded_callback\n",
      "getting result started\n",
      "BasicAsyncResult completed with result `(tensor([[[ 0.0000, ...`\n",
      "writing started\n",
      "1428.5592041015625 MB\n",
      "writing finishedsubmit task\n",
      "\n",
      "chunks_count 25\n",
      "chunk_loaded_callback\n",
      "getting result started\n",
      "BasicAsyncResult completed with result `(tensor([[[ 0.0000, ...`\n",
      "writing started\n",
      "1528.3598327636719 MB\n",
      "writing finishedsubmit task\n",
      "\n",
      "chunks_count 26\n",
      "chunk_loaded_callback\n",
      "getting result started\n",
      "BasicAsyncResult completed with result `(tensor([[[ 0.0000, ...`\n",
      "writing started\n",
      "1527.2520446777344 MB\n",
      "writing finishedsubmit task\n",
      "\n",
      "chunks_count 27\n",
      "chunk_loaded_callback\n",
      "getting result started\n",
      "BasicAsyncResult completed with result `(tensor([[[ 0.0000, ...`\n",
      "writing started\n",
      "1615.4541320800781 MB\n",
      "writing finishedsubmit task\n",
      "\n",
      "chunks_count 28\n",
      "chunk_loaded_callback\n",
      "getting result started\n",
      "BasicAsyncResult completed with result `(tensor([[[ 0.0000, ...`\n",
      "writing started\n",
      "1598.671142578125 MB\n",
      "writing finishedsubmit task\n",
      "\n",
      "chunks_count 29\n",
      "chunk_loaded_callback\n",
      "getting result started\n",
      "BasicAsyncResult completed with result `(tensor([[[ 0.0000, ...`\n",
      "writing started\n",
      "1528.0939636230469 MB\n",
      "writing finishedsubmit task\n",
      "\n",
      "chunks_count 30\n",
      "writing finishedsubmit task\n",
      "\n",
      "chunks_count 31\n",
      "chunk_loaded_callback\n",
      "Out of disk space. Remainging 4.195316314697266 GB\n",
      "Out of disk space. Remainging 4.195316314697266 GB\n",
      "writing finished\n",
      "chunks_count 32\n",
      "chunk_loaded_callback\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 109\u001b[0m\n\u001b[1;32m    107\u001b[0m stream \u001b[39m=\u001b[39m Stream(get_lenta_records())\u001b[39m.\u001b[39mlimit(\u001b[39m200_000\u001b[39m)\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m record: record\u001b[39m.\u001b[39mtext)\n\u001b[1;32m    108\u001b[0m writer \u001b[39m=\u001b[39m AsyncDatasetWriter(server)\n\u001b[0;32m--> 109\u001b[0m writer\u001b[39m.\u001b[39;49mload_dataset(stream)\n",
      "Cell \u001b[0;32mIn[2], line 96\u001b[0m, in \u001b[0;36mAsyncDatasetWriter.load_dataset\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m stream\u001b[39m.\u001b[39mgroup(params[\u001b[39m\"\u001b[39m\u001b[39mchunk_size\u001b[39m\u001b[39m\"\u001b[39m]):\n\u001b[1;32m     95\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mthread_write\u001b[39m.\u001b[39mis_alive(): \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparallel_count\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m     98\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39msubmit task\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     99\u001b[0m     future \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrpc_server\u001b[39m.\u001b[39mrpc_async(dataset_builder\u001b[39m.\u001b[39mcreate_dataset, chunk, params, \u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/micromamba/envs/pytorch-env/envs/pytorch-env/lib/python3.10/threading.py:467\u001b[0m, in \u001b[0;36mSemaphore.acquire\u001b[0;34m(self, blocking, timeout)\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[39mif\u001b[39;00m timeout \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    466\u001b[0m                 \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 467\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    468\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/micromamba/envs/pytorch-env/envs/pytorch-env/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "params[\"train_test_split\"] = 0.9\n",
    "params[\"chunk_size\"] = 3000 # 3000 # 300000\n",
    "params[\"batch_size\"] = 20000\n",
    "params[\"max_parallel\"] = 3\n",
    "params[\"type\"] = \"lenta\"\n",
    "\n",
    "params[\"max_last_read_queue_size\"] = 1\n",
    "\n",
    "\n",
    "class AsyncDatasetWriter:\n",
    "    def __init__(self, rpc_server):\n",
    "        self.storage = Storage(\"cache/storage\")\n",
    "        self.storage.clear()\n",
    "        self.chunks_count = 0\n",
    "\n",
    "        self.storage.write_meta(\"chunks_count\", 0)\n",
    "        self.storage.write_meta(\"params\", params)\n",
    "\n",
    "        self.parallel_count = threading.Semaphore(params[\"max_parallel\"])\n",
    "        self.write_queue = queue.Queue()\n",
    "        # self.thread_write = threading.Thread(target=asyncio.run, args=(self.write_task(),))\n",
    "        \n",
    "        self.device = torch.device('cuda:0')\n",
    "\n",
    "        self.chunks_iter = None\n",
    "        self.rpc_server = rpc_server\n",
    "        \n",
    "        self.thread_write = threading.Thread(target=self.write_task)\n",
    "        self.thread_write.start()\n",
    "        \n",
    "        \n",
    "    def chunk_loaded_callback(self, future):\n",
    "        print(\"chunk_loaded_callback\")\n",
    "        self.write_queue.put(future)\n",
    "\n",
    "    def has_free_space(self):\n",
    "        free_space_gb = shutil.disk_usage(self.storage.path).free / 1024 / 1024 / 1024\n",
    "        if free_space_gb < 5:\n",
    "            print(f\"Out of disk space. Remainging {free_space_gb} GB\")\n",
    "            return False\n",
    "        return True\n",
    "        \n",
    "\n",
    "    def write(self, future, i):\n",
    "        if not self.has_free_space(): return\n",
    "        # if future.exception() is not None:\n",
    "        #     future.\n",
    "            # absprint(\"ERROR: \", future.exception(), \"\\n\", \n",
    "                    # \"\\n\".join(traceback.format_tb(future.traceback())  ))\n",
    "            # raise future.exception()\n",
    "        print(\"getting result started\")\n",
    "        print(future)\n",
    "        x, y, text_res, is_infected = future.get_result()\n",
    "        print(\"writing started\")\n",
    "        print(size_of_tensor(x) / 1024 / 1024, \"MB\")\n",
    "        self.storage.store(\"x\", i, x)\n",
    "        self.storage.store(\"y\", i, y)\n",
    "        self.storage.store(\"text_res\", i, text_res)\n",
    "        self.storage.store(\"is_infected\", i, is_infected)\n",
    "        self.parallel_count.release()\n",
    "        self.write_queue.task_done()\n",
    "        print(\"writing finished\")\n",
    "        self.chunks_count += 1\n",
    "        print(\"chunks_count\", self.chunks_count)\n",
    "        self.storage.write_meta(\"chunks_count\", self.chunks_count)\n",
    "\n",
    "    def write_task(self):        \n",
    "        try:\n",
    "            print(\"write_task started\")\n",
    "            keep_running = True        \n",
    "            def handle_executor_done_callback(future):\n",
    "                nonlocal keep_running\n",
    "                try:\n",
    "                    future.result()\n",
    "                except Exception as e:\n",
    "                    print(\"ERROR\", \"writer thread failed:\\n\", type(e).__name__, e)\n",
    "                    print(\"\\n\".join(traceback.format_tb(e.__traceback__)))\n",
    "                    keep_running = False\n",
    "\n",
    "            with concurrent.futures.ThreadPoolExecutor(params[\"max_parallel\"], \"WRITER\") as executor:\n",
    "                chunk_number = 0\n",
    "                while keep_running:\n",
    "                    if not self.has_free_space(): return\n",
    "                    future = executor.submit(self.write, self.write_queue.get(), chunk_number)\n",
    "                    future.add_done_callback(handle_executor_done_callback)\n",
    "                    chunk_number += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"ERROR\", \"writer thread failed:\\n\", type(e).__name__, e)\n",
    "            print(\"\\n\".join(traceback.format_tb(e.__traceback__)))\n",
    "    \n",
    "    def load_dataset(self, stream):\n",
    "        # with concurrent.futures.ThreadPoolExecutor(max_workers=params[\"max_parallel\"]) as executor:\n",
    "        for chunk in stream.group(params[\"chunk_size\"]):\n",
    "            if not self.thread_write.is_alive(): return\n",
    "            self.parallel_count.acquire()\n",
    "\n",
    "            print(\"submit task\")\n",
    "            future = self.rpc_server.rpc_async(dataset_builder.create_dataset, chunk, params, False)\n",
    "            future.set_callback(self.chunk_loaded_callback)\n",
    "            future = None\n",
    "        \n",
    "        \n",
    "from dataset_lib import get_lenta_records\n",
    "server = run_server_if_not_running()\n",
    "server_install_packages(server)\n",
    "stream = Stream(get_lenta_records()).limit(200_000).map(lambda record: record.text)\n",
    "writer = AsyncDatasetWriter(server)\n",
    "writer.load_dataset(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d1c2771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "9\n",
      "2009\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b''"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Kek:\n",
    "    def __init__(self, a):\n",
    "        self.a = a\n",
    "    \n",
    "    def read(self, cnt=-1): \n",
    "        print(cnt)\n",
    "        return self.a.read(cnt)\n",
    "    def readline(self): raise Exception(\"no realization\")\n",
    "    def readable(self): return True\n",
    "\n",
    "    def write(self, buf):\n",
    "        print(len(buf))\n",
    "\n",
    "import io\n",
    "import dill\n",
    "\n",
    "a = Kek(io.BytesIO())\n",
    "\n",
    "dill.dump([\"kek\"] * 1000, a)\n",
    "\n",
    "# Reset the position of the BytesIO object to the beginning\n",
    "a.a.seek(0)\n",
    "\n",
    "# Pass the BytesIO object directly to the Kek class\n",
    "a.a.read()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
