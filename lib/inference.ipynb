{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mprefix\u001b[39m}\u001b[39;00m\u001b[39m/some_model_CLASS.dill\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m     22\u001b[0m     Model \u001b[39m=\u001b[39m dill\u001b[39m.\u001b[39mload(file)\n\u001b[0;32m---> 23\u001b[0m model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mprefix\u001b[39m}\u001b[39;49;00m\u001b[39m/some_model.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m, map_location\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mdevice(\u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m)) \u001b[39m#).v1\", map_location=torch.device('cuda:0'))\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mprefix\u001b[39m}\u001b[39;00m\u001b[39m/storage_path.dill\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m     26\u001b[0m     storage_path \u001b[39m=\u001b[39m dill\u001b[39m.\u001b[39mload(file)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:809\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    808\u001b[0m                 \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 809\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m    810\u001b[0m \u001b[39mif\u001b[39;00m weights_only:\n\u001b[1;32m    811\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:1172\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1170\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1171\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1172\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1174\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1176\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:1165\u001b[0m, in \u001b[0;36m_load.<locals>.UnpicklerWrapper.find_class\u001b[0;34m(self, mod_name, name)\u001b[0m\n\u001b[1;32m   1163\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1164\u001b[0m mod_name \u001b[39m=\u001b[39m load_module_mapping\u001b[39m.\u001b[39mget(mod_name, mod_name)\n\u001b[0;32m-> 1165\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfind_class(mod_name, name)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xformers'"
     ]
    }
   ],
   "source": [
    "# import imports\n",
    "# import importlib\n",
    "# importlib.reload(imports)\n",
    "# from imports import *\n",
    "import torch.nn as nn\n",
    "import dill\n",
    "import torch\n",
    "from storage import Storage\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(INTERNAL_EMBEDDING_SIZE2, INTERNAL_EMBEDDING_SIZE2 // 2, \n",
    "                            num_layers=1, batch_first=True, bidirectional=True)\n",
    "    def forward(self, x):\n",
    "        return self.lstm(x)[0]\n",
    "\n",
    "\n",
    "prefix = \"results writers big\"\n",
    "with open(f\"{prefix}/some_model_CLASS.dill\", \"rb\") as file:\n",
    "    Model = dill.load(file)\n",
    "model = torch.load(f\"{prefix}/some_model.pt\", map_location=torch.device('cpu')) #).v1\", map_location=torch.device('cuda:0'))\n",
    "\n",
    "with open(f\"{prefix}/storage_path.dill\", \"rb\") as file:\n",
    "    storage_path = dill.load(file)\n",
    "with open(f\"{prefix}/is_infected_test.dill\", \"rb\") as file:\n",
    "    is_infected = dill.load(file)\n",
    "\n",
    "storage = Storage(str(storage_path))\n",
    "params = storage.get_meta(\"params\")\n",
    "\n",
    "x_test = storage.get_meta(\"x_test\").float()\n",
    "y_test = storage.get_meta(\"y_test\").float()\n",
    "text_res = storage.get_meta(\"text_res_test\")\n",
    "is_infected = storage.get_meta(\"is_infected_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7.9433, -7.6244, -3.0897]])\n"
     ]
    }
   ],
   "source": [
    "dummy_input = x_test[0:1]\n",
    "with torch.no_grad():\n",
    "        print(model(dummy_input))\n",
    "        # torch.onnx.export(model, dummy_input, \"model.v1.onnx\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Diagnostic Run torch.onnx.export version 2.0.0 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_script_graph, unconvertible_ops = torch.onnx.utils.unconvertible_ops(\n",
    "    model, dummy_input #, opset_version=opset_version\n",
    ")\n",
    "unconvertible_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Diagnostic Run torch.onnx.export version 2.0.0 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(model, (dummy_input, ), f\"{prefix}/model.onnx\", \n",
    "                input_names = ['input'],\n",
    "                output_names = ['output'])#, verbose=True)\n",
    "        # print(torch.onnx.export_to_pretty_string(model, (dummy_input, )))#, verbose=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: tensor([[ 7.9433, -7.6244, -3.0897]])\n",
      "onnx: [[ 7.943265  -7.6243734 -3.0896897]]\n",
      "torch: tensor([[ 5.6224, -6.0555, -3.0875]])\n",
      "onnx: [[ 5.6224437 -6.0554595 -3.0875366]]\n"
     ]
    }
   ],
   "source": [
    "from inference import torch_model_runner, onnx_model_runner\n",
    "for a in [dummy_input, np.zeros_like(dummy_input)]:\n",
    "    print('torch:', torch_model_runner(model)(torch.Tensor(a)))\n",
    "    print('onnx:', onnx_model_runner(f'{prefix}/model.onnx')(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Отмечается, что 44-летняя россиянка, первый человек, которому предъявят обвинение в попытке вмешаться в выборы 2018 года. По информации Минюста она и еще несколько человек выдавала себя за американскую политическую активистку и скрывала свое российское происхождение.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from inference import torch_model_runner, onnx_model_runner, infer\n",
    "\n",
    "infer(params, onnx_model_runner(f\"{prefix}/model.onnx\"), \n",
    "      \"Отмечается что 44-летняя россиянка первый человек которому предъявят обвинение в попытке вмешаться в выборы 2018 года По информации Минюста она и еще несколько человек выдавала себя за американскую политическую активистку и скрывала свое российское происхождение\")\n",
    "# print(infer(torch_model_runner(model), \"Я пришел домой Мама сварила суп\"))\n",
    "# https://onnxruntime.ai/docs/reference/ort-format-models.html\n",
    "\n",
    "# infer(onnx_model_runner(\"model.v1.onnx\"), \"Я пришел домой Мама сварила суп\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Мама обычно холодная, как оконное стекло, была в такой ярости, что переколотила дома всю посуду. Хотела даже просить расчёт, но опомнилась: в других ресторанах и своих певиц хватало. Потому мама просто перестала брать Саню с собой, и он впервые оказался предоставлен самому себе. Поначалу Саня робел, один выйти наружу, скучал, слонялся, не зная, чем заняться, — то по комнатам, то по двору. Но к весне осмелел настолько, что исследовал сперва окрестные улицы, а потом и весь обитаемый воронежский мир. Мир этот оказался полон женщинами. Саня, разинув от восхищения рот, смотрел на проплывающих по Большой Дворянской разодетых дам, похожих на вазы с фруктами и цветами. На гимназисток, вечно державшихся дрожащими стайками, словно мотыльки. На горничных, девушек, спешащих по хозяйкиным делам и прошивающих город мелкими аккуратными стежками. Саня влюбился во всех них сразу — и навсегда.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer(params, onnx_model_runner(f\"{prefix}/model.onnx\"), \n",
    "      text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parse(word='фывфывфов', tag=OpencorporaTag('NOUN,inan,masc plur,gent'), normal_form='фывфывф', score=1.0, methods_stack=((FakeDictionary(), 'фывфывфов', 34, 7), (KnownSuffixAnalyzer(min_word_length=4, score_multiplier=0.5), 'фы')))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "morph.parse(\"фывфывфы\")[0].inflect({'gent'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"\"\"\n",
    "# Кристина Герасимова (к концу десятилетия, кажется, поменявшая фамилию на Луна) к концу 2010-х оказалась одним из самых работоспособных музыкантов в русской поп-музыке (впрочем, и не только поп), выйдя на уровень, при котором она смогла себе позволить выпускать по альбому в год без потерь в качестве. Лучший из них, впрочем, «Заколдованные сны», как будто бы нарушал классическую для нее схему и уходил то в чистый и практически бессловесный электропоп, то в ностальгию по неприхотливой русской поп-музыке нулевых, то в постпанк — при этом всегда оставаясь песнями самой Луны.\n",
    "# \"\"\"\n",
    "text = \"\"\"\n",
    "\n",
    "Мама, обычно холодная, как оконное стекло, была в такой ярости, что переколотила дома всю посуду. Хотела даже просить расчёт, но опомнилась: в других ресторанах и своих певиц хватало. Потому мама просто перестала брать Саню с собой, и он впервые оказался предоставлен самому себе. Поначалу Саня робел один выйти наружу, скучал, слонялся, не зная, чем заняться, — то по комнатам, то по двору. Но к весне осмелел настолько, что исследовал сперва окрестные улицы, а потом и весь обитаемый воронежский мир.\n",
    "\n",
    "Мир этот оказался полон женщинами.\n",
    "\n",
    "Саня, разинув от восхищения рот, смотрел на проплывающих по Большой Дворянской разодетых дам, похожих на вазы с фруктами и цветами. На гимназисток, вечно державшихся дрожащими стайками, словно мотыльки. На горничных девушек, спешащих по хозяйкиным делам и прошивающих город мелкими аккуратными стежками.\n",
    "\n",
    "Саня влюбился во всех них сразу — и навсегда.\"\"\"\n",
    "#text = text.replace(\"\\n\", \" \")\n",
    "# print(text)\n",
    "\n",
    "# text = text.replace(\".\", \"\").replace(\",\", \"\").replace(\"\\n\", \" \")\n",
    "# infer(params, onnx_model_runner(f\"{prefix}/model.onnx\"), text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Мама',\n",
       " ',',\n",
       " 'обычно',\n",
       " 'холодная',\n",
       " ',',\n",
       " 'как',\n",
       " 'оконное',\n",
       " 'стекло',\n",
       " ',',\n",
       " 'была',\n",
       " 'в',\n",
       " 'такой',\n",
       " 'ярости',\n",
       " ',',\n",
       " 'что',\n",
       " 'переколотила',\n",
       " 'дома',\n",
       " 'всю',\n",
       " 'посуду',\n",
       " '.',\n",
       " 'Хотела',\n",
       " 'даже',\n",
       " 'просить',\n",
       " 'расчёт',\n",
       " ',',\n",
       " 'но',\n",
       " 'опомнилась',\n",
       " ':',\n",
       " 'в',\n",
       " 'других',\n",
       " 'ресторанах',\n",
       " 'и',\n",
       " 'своих',\n",
       " 'певиц',\n",
       " 'хватало',\n",
       " '.',\n",
       " 'Потому',\n",
       " 'мама',\n",
       " 'просто',\n",
       " 'перестала',\n",
       " 'брать',\n",
       " 'Саню',\n",
       " 'с',\n",
       " 'собой',\n",
       " ',',\n",
       " 'и',\n",
       " 'он',\n",
       " 'впервые',\n",
       " 'оказался',\n",
       " 'предоставлен',\n",
       " 'самому',\n",
       " 'себе',\n",
       " '.',\n",
       " 'Поначалу',\n",
       " 'Саня',\n",
       " 'робел',\n",
       " 'один',\n",
       " 'выйти',\n",
       " 'наружу',\n",
       " ',',\n",
       " 'скучал',\n",
       " ',',\n",
       " 'слонялся',\n",
       " ',',\n",
       " 'не',\n",
       " 'зная',\n",
       " ',',\n",
       " 'чем',\n",
       " 'заняться',\n",
       " ',',\n",
       " '—',\n",
       " 'то',\n",
       " 'по',\n",
       " 'комнатам',\n",
       " ',',\n",
       " 'то',\n",
       " 'по',\n",
       " 'двору',\n",
       " '.',\n",
       " 'Но',\n",
       " 'к',\n",
       " 'весне',\n",
       " 'осмелел',\n",
       " 'настолько',\n",
       " ',',\n",
       " 'что',\n",
       " 'исследовал',\n",
       " 'сперва',\n",
       " 'окрестные',\n",
       " 'улицы',\n",
       " ',',\n",
       " 'а',\n",
       " 'потом',\n",
       " 'и',\n",
       " 'весь',\n",
       " 'обитаемый',\n",
       " 'воронежский',\n",
       " 'мир',\n",
       " '.',\n",
       " 'Мир',\n",
       " 'этот',\n",
       " 'оказался',\n",
       " 'полон',\n",
       " 'женщинами',\n",
       " '.',\n",
       " 'Саня',\n",
       " ',',\n",
       " 'разинув',\n",
       " 'от',\n",
       " 'восхищения',\n",
       " 'рот',\n",
       " ',',\n",
       " 'смотрел',\n",
       " 'на',\n",
       " 'проплывающих',\n",
       " 'по',\n",
       " 'Большой',\n",
       " 'Дворянской',\n",
       " 'разодетых',\n",
       " 'дам',\n",
       " ',',\n",
       " 'похожих',\n",
       " 'на',\n",
       " 'вазы',\n",
       " 'с',\n",
       " 'фруктами',\n",
       " 'и',\n",
       " 'цветами',\n",
       " '.',\n",
       " 'На',\n",
       " 'гимназисток',\n",
       " ',',\n",
       " 'вечно',\n",
       " 'державшихся',\n",
       " 'дрожащими',\n",
       " 'стайками',\n",
       " ',',\n",
       " 'словно',\n",
       " 'мотыльки',\n",
       " '.',\n",
       " 'На',\n",
       " 'горничных',\n",
       " 'девушек',\n",
       " ',',\n",
       " 'спешащих',\n",
       " 'по',\n",
       " 'хозяйкиным',\n",
       " 'делам',\n",
       " 'и',\n",
       " 'прошивающих',\n",
       " 'город',\n",
       " 'мелкими',\n",
       " 'аккуратными',\n",
       " 'стежками',\n",
       " '.',\n",
       " 'Саня',\n",
       " 'влюбился',\n",
       " 'во',\n",
       " 'всех',\n",
       " 'них',\n",
       " 'сразу',\n",
       " '—',\n",
       " 'и',\n",
       " 'навсегда',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from razdel import tokenize\n",
    "list(map(lambda i: i.text, tokenize(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m count_parameters\n\u001b[0;32m----> 2\u001b[0m count_parameters(model)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172.7963150459843 tokens per second\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "start = time()\n",
    "reps = 100\n",
    "for i in range(0, reps):\n",
    "    infer(onnx_model_runner(\"model.v1.onnx\"), text)\n",
    "end = time()\n",
    "\n",
    "print(1 / ((end - start) / reps / len(text.split(' '))) / 8, 'tokens per second')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129.86710276061967 tokens per second\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "dummy_input = torch.ones((1, params['INPUT_WORDS_CNT'], params['TOTAL_WORD_FEATURES_CNT']))\n",
    "model.eval()\n",
    "\n",
    "start = time()\n",
    "reps = 10000\n",
    "for i in range(0, reps):\n",
    "    with torch.no_grad():\n",
    "        model(dummy_input)\n",
    "end = time()\n",
    "\n",
    "print(1 / ((end - start) / reps) / 8, 'tokens per second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19739913940429688"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import count_parameters\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'Model' on <module '__main__'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39mmodel.v1\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m model\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:789\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    787\u001b[0m             \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    788\u001b[0m                 \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 789\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m    790\u001b[0m \u001b[39mif\u001b[39;00m weights_only:\n\u001b[1;32m    791\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:1131\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1130\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1131\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1133\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1135\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:1124\u001b[0m, in \u001b[0;36m_load.<locals>.UnpicklerWrapper.find_class\u001b[0;34m(self, mod_name, name)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m mod_name \u001b[39m=\u001b[39m load_module_mapping\u001b[39m.\u001b[39mget(mod_name, mod_name)\n\u001b[0;32m-> 1124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfind_class(mod_name, name)\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't get attribute 'Model' on <module '__main__'>"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model = torch.load(\"model.v1\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD <$empty> Привет ! Как дела ? Все хорошо . PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD Привет <.> ! Как дела ? Все хорошо . PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD Привет ! Как <$empty> дела ? Все хорошо . PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD Привет ! Как дела <.> ? Все хорошо . PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD Привет ! Как дела ? Все <$empty> хорошо . PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "PAD PAD PAD PAD PAD PAD PAD PAD PAD Привет ! Как дела ? Все хорошо <.> . PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Привет! Как дела? Все хорошо.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pymorphy3\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "from razdel import tokenize\n",
    "\n",
    "from dataset_builder import calculate_word_features_for_tokens, PAD_TOKEN,get_word_features\n",
    "from inference import torch_model_runner, onnx_model_runner, infer\n",
    "\n",
    "onnx_model = onnx_model_runner(\"results writers big/model.onnx\")\n",
    "with open(\"params.pickle\", \"rb\") as f:\n",
    "    params = pickle.load(f)\n",
    "\n",
    "class jsinfer:\n",
    "    async def infer(arr):\n",
    "        class wrapper:\n",
    "            def to_py():\n",
    "                return onnx_model(arr)\n",
    "        return wrapper\n",
    "\n",
    "\n",
    "async def infer(text):\n",
    "    assert params[\"RETAIN_LEFT_PUNCT\"] #\n",
    "\n",
    "    #unpadded_tokens = text.split(' ')\n",
    "    unpadded_tokens = list(map(lambda i: i.text, tokenize(text)))\n",
    "    unpadded_tokens = list(filter(lambda x: len(x) > 0, unpadded_tokens))\n",
    "    tokens = [PAD_TOKEN] * params['INPUT_WORDS_CNT_LEFT'] + unpadded_tokens + [PAD_TOKEN] * (params[\"INPUT_WORDS_CNT_RIGHT\"] + 1)\n",
    "    features = calculate_word_features_for_tokens(tokens, params)\n",
    "\n",
    "    res = \"\"\n",
    "\n",
    "    i = params['INPUT_WORDS_CNT_LEFT']\n",
    "    while i < len(tokens) - params['INPUT_WORDS_CNT_RIGHT']:\n",
    "        tokens_for_batch = tokens[i - params['INPUT_WORDS_CNT_LEFT']: i + params['INPUT_WORDS_CNT_RIGHT']]\n",
    "\n",
    "        tokens_for_batch_copy = tokens_for_batch.copy()\n",
    "        tokens_for_batch_copy.insert(params['INPUT_WORDS_CNT_LEFT'], '?')\n",
    "        # print(\" \".join(tokens_for_batch_copy))\n",
    "\n",
    "\n",
    "        features_for_batch = features[i - params['INPUT_WORDS_CNT_LEFT']: i + params['INPUT_WORDS_CNT_RIGHT']]\n",
    "        features_for_batch = np.stack((features_for_batch, ))\n",
    "        arr = np.ascontiguousarray(features_for_batch, dtype=np.float32)\n",
    "        output_probas = np.array((await jsinfer.infer(arr)).to_py())\n",
    "        punct_idx = np.argmax(output_probas).item()\n",
    "        punct = params[\"ID_TO_PUNCTUATION\"][punct_idx]\n",
    "\n",
    "        #tokens_for_batch_copy[params['INPUT_WORDS_CNT_LEFT']] =  f'<{tokens[i-1], punct, tokens[i]}>'\n",
    "        tokens_for_batch_copy[params['INPUT_WORDS_CNT_LEFT']] =  f'<{punct}>'\n",
    "        print(\" \".join(tokens_for_batch_copy))\n",
    "\n",
    "        # print(punct)\n",
    "        # punct = '.'\n",
    "\n",
    "        def add_i_token_to_res(i):\n",
    "            nonlocal res\n",
    "            if tokens[i] != 'PAD':\n",
    "                if tokens[i] in '.,:!?':\n",
    "                    res += tokens[i]\n",
    "                else:\n",
    "                    res += \" \" + tokens[i]\n",
    "\n",
    "        def insert_punct(punct):\n",
    "            nonlocal features\n",
    "            tokens.insert(i, punct)\n",
    "            features = np.concatenate((features[:i],\n",
    "                    np.stack((get_word_features(punct, params), )),\n",
    "                    features[i:]), 0)\n",
    "\n",
    "        def replace_i_token_punct(punct):\n",
    "            nonlocal features\n",
    "            tokens[i] = punct\n",
    "            features[i] = get_word_features(punct, params)\n",
    "\n",
    "        def erase_i_token_punct():\n",
    "            nonlocal features\n",
    "            del tokens[i]\n",
    "            features = np.concatenate((features[:i], features[i + 1:]), 0)\n",
    "\n",
    "        if tokens[i] in '.,!?':\n",
    "            if tokens[i] == punct or tokens[i] in \"!?\":# and punct in \".\"):\n",
    "                add_i_token_to_res(i)\n",
    "                add_i_token_to_res(i + 1)\n",
    "                i += 2\n",
    "            else:\n",
    "                if punct != \"$empty\":\n",
    "                    res += punct\n",
    "                    replace_i_token_punct(punct)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    # res += \" \"\n",
    "                    erase_i_token_punct()\n",
    "                    # i += 0\n",
    "\n",
    "        else:\n",
    "            if punct != \"$empty\":\n",
    "                res += punct #\"{ins: \" + punct + \"`\" + tokens[i] + \"`}\"\n",
    "                add_i_token_to_res(i)\n",
    "                insert_punct(punct)\n",
    "                i += 2\n",
    "            else:\n",
    "                add_i_token_to_res(i)\n",
    "                i += 1\n",
    "\n",
    "    return res.strip()\n",
    "\n",
    "await infer(\"Привет! Как дела? Все хорошо.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
