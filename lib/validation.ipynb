{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pymorphy3\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "from razdel import tokenize\n",
    "\n",
    "from dataset_builder import calculate_word_features_for_tokens, PAD_TOKEN,get_word_features\n",
    "from inference import torch_model_runner, onnx_model_runner, infer\n",
    "\n",
    "onnx_model = onnx_model_runner(\"results writers big/model.onnx\")\n",
    "with open(\"params.pickle\", \"rb\") as f:\n",
    "    params = pickle.load(f)\n",
    "\n",
    "class jsinfer:\n",
    "    async def infer(arr):\n",
    "        class wrapper:\n",
    "            def to_py():\n",
    "                return onnx_model(arr)\n",
    "        return wrapper\n",
    "\n",
    "from stream import Stream\n",
    "import functools\n",
    "from collections import deque\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "@functools.lru_cache(maxsize=128)\n",
    "def get_word_features_cached(word):\n",
    "    return get_word_features(word, params).numpy()\n",
    "\n",
    "class Substr:\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Substring(-1, -1, {self.text})\"\n",
    "\n",
    "def d_as_str(d):\n",
    "  return \"<\" + \" \".join(map(lambda text: text.text, d))+ \">\"\n",
    "    \n",
    "async def infer_optimal(params, text): \n",
    "  # print(\"INFERCENC IS WIERD\\n\" * 10)\n",
    "  res = []\n",
    "  last_inserted_pos = 0\n",
    "  def sink(token, log=False):\n",
    "    nonlocal last_inserted_pos\n",
    "    if token.text == \"PAD\": return\n",
    "    if log: print('sink', token)\n",
    "    if isinstance(token, Substr):\n",
    "      res.append(token.text)\n",
    "      if log: print(\"added1 \", f\"`{token.text}`\", token)\n",
    "    else:\n",
    "      if last_inserted_pos != token.start:\n",
    "        res.append(text[last_inserted_pos: token.start])\n",
    "        if log: print(\"added2 \", f\"`{text[last_inserted_pos: token.start]}`\", last_inserted_pos, token.start)\n",
    "      last_inserted_pos = token.stop\n",
    "      res.append(token.text)\n",
    "      if log: print(\"added3 \", f\"`{token.text}`\", token)\n",
    "\n",
    "  def skip(token, log=False):\n",
    "    nonlocal last_inserted_pos\n",
    "    last_inserted_pos = token.stop\n",
    "    if log: print('skip', token)\n",
    "\n",
    "  def sink_remaining():\n",
    "     res.append(text[last_inserted_pos:])\n",
    "\n",
    "\n",
    "  async def predict_on_tokens(window_left, window_right, return_probas):\n",
    "    features = [get_word_features_cached(i.text) for i in Stream(window_left).chain(window_right)]\n",
    "    features_for_batch = np.stack((features, ))\n",
    "    arr = np.ascontiguousarray(features_for_batch, dtype=np.float32)\n",
    "    output_probas = np.array((await jsinfer.infer(arr)).to_py())\n",
    "    # output_probas[0][0] += 2.\n",
    "    if return_probas:\n",
    "      return params[\"ID_TO_PUNCTUATION\"], output_probas \n",
    "    punct_idx = np.argmax(output_probas).item()\n",
    "    punct = params[\"ID_TO_PUNCTUATION\"][punct_idx]\n",
    "    return punct\n",
    "\n",
    "\n",
    "  window_left = deque()\n",
    "  window_right = deque()\n",
    "  log = False\n",
    "  skip_next = False\n",
    "  for i in Stream.repeat(Substr(PAD_TOKEN), params['INPUT_WORDS_CNT_LEFT']) \\\n",
    "      .chain(Stream(tokenize(text))) \\\n",
    "      .chain(Stream.repeat(Substr(PAD_TOKEN), params[\"INPUT_WORDS_CNT_RIGHT\"])):\n",
    "    window_right.append(i)\n",
    "    if len(window_right) <= params[\"INPUT_WORDS_CNT_RIGHT\"]:\n",
    "        continue\n",
    "    assert len(window_right) == params[\"INPUT_WORDS_CNT_RIGHT\"] + 1\n",
    "\n",
    "    next_ = window_right.popleft()\n",
    "    sink(next_)\n",
    "    window_left.append(next_)\n",
    "    if len(window_left) < params['INPUT_WORDS_CNT_LEFT']: \n",
    "      continue\n",
    "\n",
    "    assert len(window_left) == params[\"INPUT_WORDS_CNT_LEFT\"]\n",
    "    assert len(window_right) == params[\"INPUT_WORDS_CNT_RIGHT\"]\n",
    "\n",
    "    if skip_next:\n",
    "      prediction = \"$skip\" \n",
    "    else:\n",
    "      # params[\"ID_TO_PUNCTUATION\"], output_probas\n",
    "      prediction = await predict_on_tokens(window_left, window_right, return_probas=False) \n",
    "\n",
    "\n",
    "    #random.choice([\" \", \".\"])\n",
    "    if log: print(d_as_str(window_left).rjust(100), prediction.center(6), d_as_str(window_right))\n",
    "\n",
    "    def is_replaceable_punct(punct):\n",
    "      return punct in ',.'\n",
    "\n",
    "    if prediction == \"$skip\":\n",
    "      pass\n",
    "    elif prediction != \"$empty\":\n",
    "      if is_replaceable_punct(window_right[0].text):\n",
    "        if window_right[0].text != prediction:\n",
    "          window_right[0].text = prediction\n",
    "      else:\n",
    "        window_left.append(Substr(prediction))\n",
    "        sink(window_left[-1])\n",
    "    else:\n",
    "      if is_replaceable_punct(window_right[0].text):\n",
    "          skip(window_right.popleft())\n",
    "\n",
    "    skip_next = is_replaceable_punct(window_right[0].text)\n",
    "\n",
    "    while len(window_left) != params['INPUT_WORDS_CNT_LEFT'] - 1: \n",
    "      token = window_left.popleft()\n",
    "\n",
    "    if log: print(d_as_str(window_left).rjust(100), \"      \", d_as_str(window_right))\n",
    "\n",
    "  for i in window_right:\n",
    "    sink(i)\n",
    "  sink_remaining()\n",
    "  ress = \"\".join(res)\n",
    "  return ress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'кек.\\n'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await infer_optimal(params, \"кек\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.calculate_diff.<locals>.<lambda>()>,\n",
       "            {'unchanged .': 0, 'unchanged ,': 0, 'add .': 1})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import diff_match_patch as dmp_module\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "def calculate_diff(text, text_res):\n",
    "  dmp = dmp_module.diff_match_patch()\n",
    "  diff = dmp.diff_main(text, text_res)\n",
    "\n",
    "  diff_aggregate = defaultdict(lambda : 0)\n",
    "  def sink2():\n",
    "    nonlocal cur_add, cur_remove\n",
    "    if cur_add == \"\" and cur_remove == \"\":\n",
    "      return\n",
    "    if cur_add == \"\":\n",
    "      diff_aggregate['remove ' + cur_remove] += 1\n",
    "      cur_remove = \"\"\n",
    "      return\n",
    "    if cur_remove == \"\":\n",
    "      diff_aggregate['add ' + cur_add] += 1\n",
    "      cur_add = \"\"\n",
    "      return\n",
    "    \n",
    "    diff_aggregate['replace ' + cur_remove + \" with \" + cur_add] += 1\n",
    "    cur_add = \"\"\n",
    "    cur_remove = \"\"\n",
    "\n",
    "  cur_remove = \"\"\n",
    "  cur_add = \"\"\n",
    "\n",
    "  UNCHANGED = 0\n",
    "  ADD = 1\n",
    "  REMOVE = -1\n",
    "\n",
    "  for change in diff:\n",
    "      if change[0] == UNCHANGED:\n",
    "        c = Counter(change[1])\n",
    "        diff_aggregate['unchanged .'] += c['.']\n",
    "        diff_aggregate['unchanged ,'] += c[',']\n",
    "        sink2() \n",
    "      elif change[0] == ADD:\n",
    "        cur_add += change[1]\n",
    "      elif change[0] == REMOVE:\n",
    "        cur_remove += change[1]\n",
    "      else:\n",
    "        raise Exception(\"Unknown format\")\n",
    "      \n",
    "  sink2()\n",
    "\n",
    "  return diff_aggregate\n",
    " \n",
    "text = \"кек\"\n",
    "calculate_diff(text, await infer_optimal(params, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = txt\n",
    "# text_res = await infer_optimal(params, text)\n",
    "# print(text)\n",
    "# print(\"=\"* 50)\n",
    "# print(text_res)\n",
    "# diff = calculate_diff2(text, text_res)\n",
    "# diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.calculate_diff2.<locals>.<lambda>()>,\n",
       "            {'changed . with ,': 1,\n",
       "             'not changed ,': 1,\n",
       "             'not changed .': 1,\n",
       "             'added .': 1,\n",
       "             'possible punctuation places': 7})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "text = \"Тест. тест, тест. Тест\"\n",
    "text_res = await infer_optimal(params, text)\n",
    "\n",
    "def calculate_diff2(text, text_res):\n",
    "  res = defaultdict(lambda: 0)\n",
    "\n",
    "  def is_punctuation(c):\n",
    "      return c in \".,\"\n",
    "\n",
    "  def sink_add(c):\n",
    "    nonlocal res\n",
    "    res['added ' + c] += 1\n",
    "\n",
    "  def sink_remove(c):\n",
    "    nonlocal res\n",
    "    res['removed ' + c] += 1\n",
    "\n",
    "  def sink_change(c1, c2):\n",
    "    nonlocal res\n",
    "    res['changed ' + c1 + \" with \" + c2] += 1\n",
    "\n",
    "  i = 0\n",
    "  j = 0\n",
    "  while True:\n",
    "      if i >= len(text): break\n",
    "      if j >= len(text_res): break\n",
    "      # print(text[i], text_res[j])\n",
    "      if text[i] == text_res[j]:\n",
    "          if is_punctuation(text[i]):\n",
    "             res['not changed ' + text[i]] += 1\n",
    "          i += 1\n",
    "          j += 1\n",
    "          continue\n",
    "      \n",
    "      if is_punctuation(text[i]) and is_punctuation(text_res[j]):\n",
    "        sink_change(text[i], text_res[j])\n",
    "        i += 1\n",
    "        j += 1\n",
    "        continue\n",
    "      \n",
    "      if is_punctuation(text[i]):\n",
    "        sink_remove(text[i])\n",
    "        i += 1\n",
    "        continue\n",
    "      \n",
    "      if is_punctuation(text_res[j]):\n",
    "        sink_add(text_res[j])\n",
    "        j += 1\n",
    "        continue\n",
    "      \n",
    "      raise Exception(\"Change not in punctuation\", text[i], text_res[j], \"at \", i, j)\n",
    "\n",
    "  while i < len(text):\n",
    "    # print(\"remaining: \", text[i])\n",
    "    assert is_punctuation(text[i])\n",
    "    sink_remove(text[i])\n",
    "    i += 1\n",
    "\n",
    "  while j < len(text_res):\n",
    "    # print(\"remaining(2): \",text_res[j])\n",
    "    assert is_punctuation(text_res[j])\n",
    "    sink_add(text_res[j])\n",
    "    j += 1\n",
    "\n",
    "  res['possible punctuation places'] = len(list(tokenize(text)))\n",
    "\n",
    "  return res\n",
    "\n",
    "calculate_diff2(text, text_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Тест. тест, тест. Тест', 'Тест, тест, тест. Тест.')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text, text_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "679fe4ed04eb44998451aa8703e6a186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped  ../validation/Mark Tven/Mark Tven rtf/Tven_Iz_zapisnyih_knizhek_1865-1905.131699.rtf 125718 ('Change not in punctuation', ' ', '\\n', 'at ', 23961, 24102)\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "('Change not in punctuation', ' ', '\\n', 'at ', 23961, 24102)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m   rtf \u001b[39m=\u001b[39m encoded\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mcp1251\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m   txt \u001b[39m=\u001b[39m rtf_to_text(rtf)\n\u001b[0;32m---> 19\u001b[0m   diff \u001b[39m=\u001b[39m calculate_diff2(txt, \u001b[39mawait\u001b[39;00m infer_optimal(params, txt))\n\u001b[1;32m     20\u001b[0m   res \u001b[39m=\u001b[39m dicts_sum(res, diff)\n\u001b[1;32m     21\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m ex:\n",
      "Cell \u001b[0;32mIn[2], line 52\u001b[0m, in \u001b[0;36mcalculate_diff2\u001b[0;34m(text, text_res)\u001b[0m\n\u001b[1;32m     49\u001b[0m       j \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     50\u001b[0m       \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mChange not in punctuation\u001b[39m\u001b[39m\"\u001b[39m, text[i], text_res[j], \u001b[39m\"\u001b[39m\u001b[39mat \u001b[39m\u001b[39m\"\u001b[39m, i, j)\n\u001b[1;32m     54\u001b[0m \u001b[39mwhile\u001b[39;00m i \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(text):\n\u001b[1;32m     55\u001b[0m   \u001b[39m# print(\"remaining: \", text[i])\u001b[39;00m\n\u001b[1;32m     56\u001b[0m   \u001b[39massert\u001b[39;00m is_punctuation(text[i])\n",
      "\u001b[0;31mException\u001b[0m: ('Change not in punctuation', ' ', '\\n', 'at ', 23961, 24102)"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from striprtf.striprtf import rtf_to_text\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def dicts_sum(dict1, dict2):\n",
    "  for key in dict2:\n",
    "    dict1[key] += dict2[key]\n",
    "  return dict1\n",
    "\n",
    "res = defaultdict(lambda: 0)\n",
    "\n",
    "i = 0\n",
    "for rtf_path in tqdm(glob.glob(\"../validation/Mark Tven/Mark Tven rtf/*.rtf\")):\n",
    "  with open(rtf_path, \"rb\") as rtf_file:\n",
    "    encoded = rtf_file.read()\n",
    "    try:\n",
    "      rtf = encoded.decode('cp1251')\n",
    "      txt = rtf_to_text(rtf)\n",
    "      diff = calculate_diff2(txt, await infer_optimal(params, txt))\n",
    "      res = dicts_sum(res, diff)\n",
    "    except Exception as ex:\n",
    "      print(\"skipped \", rtf_path, len(encoded), ex)\n",
    "      raise\n",
    "    i += 1\n",
    "    # if i> 2: break\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>, {})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
